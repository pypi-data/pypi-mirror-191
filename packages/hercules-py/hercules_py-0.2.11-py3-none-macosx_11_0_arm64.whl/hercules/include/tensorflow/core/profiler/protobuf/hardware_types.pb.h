// Generated by the protocol buffer compiler.  DO NOT EDIT!
// source: tensorflow/core/profiler/protobuf/hardware_types.proto

#ifndef GOOGLE_PROTOBUF_INCLUDED_tensorflow_2fcore_2fprofiler_2fprotobuf_2fhardware_5ftypes_2eproto
#define GOOGLE_PROTOBUF_INCLUDED_tensorflow_2fcore_2fprofiler_2fprotobuf_2fhardware_5ftypes_2eproto

#include <limits>
#include <string>

#include <google/protobuf/port_def.inc>
#if PROTOBUF_VERSION < 3019000
#error This file was generated by a newer version of protoc which is
#error incompatible with your Protocol Buffer headers. Please update
#error your headers.
#endif
#if 3019004 < PROTOBUF_MIN_PROTOC_VERSION
#error This file was generated by an older version of protoc which is
#error incompatible with your Protocol Buffer headers. Please
#error regenerate this file with a newer version of protoc.
#endif

#include <google/protobuf/port_undef.inc>
#include <google/protobuf/io/coded_stream.h>
#include <google/protobuf/arena.h>
#include <google/protobuf/arenastring.h>
#include <google/protobuf/generated_message_table_driven.h>
#include <google/protobuf/generated_message_util.h>
#include <google/protobuf/metadata_lite.h>
#include <google/protobuf/generated_message_reflection.h>
#include <google/protobuf/message.h>
#include <google/protobuf/repeated_field.h>  // IWYU pragma: export
#include <google/protobuf/extension_set.h>  // IWYU pragma: export
#include <google/protobuf/generated_enum_reflection.h>
#include <google/protobuf/unknown_field_set.h>
// @@protoc_insertion_point(includes)
#include <google/protobuf/port_def.inc>
#define PROTOBUF_INTERNAL_EXPORT_tensorflow_2fcore_2fprofiler_2fprotobuf_2fhardware_5ftypes_2eproto
PROTOBUF_NAMESPACE_OPEN
namespace internal {
class AnyMetadata;
}  // namespace internal
PROTOBUF_NAMESPACE_CLOSE

// Internal implementation detail -- do not use these members.
struct TableStruct_tensorflow_2fcore_2fprofiler_2fprotobuf_2fhardware_5ftypes_2eproto {
  static const ::PROTOBUF_NAMESPACE_ID::internal::ParseTableField entries[]
    PROTOBUF_SECTION_VARIABLE(protodesc_cold);
  static const ::PROTOBUF_NAMESPACE_ID::internal::AuxiliaryParseTableField aux[]
    PROTOBUF_SECTION_VARIABLE(protodesc_cold);
  static const ::PROTOBUF_NAMESPACE_ID::internal::ParseTable schema[2]
    PROTOBUF_SECTION_VARIABLE(protodesc_cold);
  static const ::PROTOBUF_NAMESPACE_ID::internal::FieldMetadata field_metadata[];
  static const ::PROTOBUF_NAMESPACE_ID::internal::SerializationTable serialization_table[];
  static const uint32_t offsets[];
};
extern const ::PROTOBUF_NAMESPACE_ID::internal::DescriptorTable descriptor_table_tensorflow_2fcore_2fprofiler_2fprotobuf_2fhardware_5ftypes_2eproto;
namespace tensorflow {
namespace profiler {
class DeviceCapabilities;
struct DeviceCapabilitiesDefaultTypeInternal;
extern DeviceCapabilitiesDefaultTypeInternal _DeviceCapabilities_default_instance_;
class GPUComputeCapability;
struct GPUComputeCapabilityDefaultTypeInternal;
extern GPUComputeCapabilityDefaultTypeInternal _GPUComputeCapability_default_instance_;
}  // namespace profiler
}  // namespace tensorflow
PROTOBUF_NAMESPACE_OPEN
template<> ::tensorflow::profiler::DeviceCapabilities* Arena::CreateMaybeMessage<::tensorflow::profiler::DeviceCapabilities>(Arena*);
template<> ::tensorflow::profiler::GPUComputeCapability* Arena::CreateMaybeMessage<::tensorflow::profiler::GPUComputeCapability>(Arena*);
PROTOBUF_NAMESPACE_CLOSE
namespace tensorflow {
namespace profiler {

enum HardwareType : int {
  UNKNOWN_HARDWARE = 0,
  CPU_ONLY = 1,
  GPU = 2,
  TPU = 3,
  HardwareType_INT_MIN_SENTINEL_DO_NOT_USE_ = std::numeric_limits<int32_t>::min(),
  HardwareType_INT_MAX_SENTINEL_DO_NOT_USE_ = std::numeric_limits<int32_t>::max()
};
bool HardwareType_IsValid(int value);
constexpr HardwareType HardwareType_MIN = UNKNOWN_HARDWARE;
constexpr HardwareType HardwareType_MAX = TPU;
constexpr int HardwareType_ARRAYSIZE = HardwareType_MAX + 1;

const ::PROTOBUF_NAMESPACE_ID::EnumDescriptor* HardwareType_descriptor();
template<typename T>
inline const std::string& HardwareType_Name(T enum_t_value) {
  static_assert(::std::is_same<T, HardwareType>::value ||
    ::std::is_integral<T>::value,
    "Incorrect type passed to function HardwareType_Name.");
  return ::PROTOBUF_NAMESPACE_ID::internal::NameOfEnum(
    HardwareType_descriptor(), enum_t_value);
}
inline bool HardwareType_Parse(
    ::PROTOBUF_NAMESPACE_ID::ConstStringParam name, HardwareType* value) {
  return ::PROTOBUF_NAMESPACE_ID::internal::ParseNamedEnum<HardwareType>(
    HardwareType_descriptor(), name, value);
}
// ===================================================================

class GPUComputeCapability final :
    public ::PROTOBUF_NAMESPACE_ID::Message /* @@protoc_insertion_point(class_definition:tensorflow.profiler.GPUComputeCapability) */ {
 public:
  inline GPUComputeCapability() : GPUComputeCapability(nullptr) {}
  ~GPUComputeCapability() override;
  explicit constexpr GPUComputeCapability(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  GPUComputeCapability(const GPUComputeCapability& from);
  GPUComputeCapability(GPUComputeCapability&& from) noexcept
    : GPUComputeCapability() {
    *this = ::std::move(from);
  }

  inline GPUComputeCapability& operator=(const GPUComputeCapability& from) {
    CopyFrom(from);
    return *this;
  }
  inline GPUComputeCapability& operator=(GPUComputeCapability&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const ::PROTOBUF_NAMESPACE_ID::Descriptor* descriptor() {
    return GetDescriptor();
  }
  static const ::PROTOBUF_NAMESPACE_ID::Descriptor* GetDescriptor() {
    return default_instance().GetMetadata().descriptor;
  }
  static const ::PROTOBUF_NAMESPACE_ID::Reflection* GetReflection() {
    return default_instance().GetMetadata().reflection;
  }
  static const GPUComputeCapability& default_instance() {
    return *internal_default_instance();
  }
  static inline const GPUComputeCapability* internal_default_instance() {
    return reinterpret_cast<const GPUComputeCapability*>(
               &_GPUComputeCapability_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    0;

  friend void swap(GPUComputeCapability& a, GPUComputeCapability& b) {
    a.Swap(&b);
  }
  inline void Swap(GPUComputeCapability* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(GPUComputeCapability* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  GPUComputeCapability* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<GPUComputeCapability>(arena);
  }
  using ::PROTOBUF_NAMESPACE_ID::Message::CopyFrom;
  void CopyFrom(const GPUComputeCapability& from);
  using ::PROTOBUF_NAMESPACE_ID::Message::MergeFrom;
  void MergeFrom(const GPUComputeCapability& from);
  private:
  static void MergeImpl(::PROTOBUF_NAMESPACE_ID::Message* to, const ::PROTOBUF_NAMESPACE_ID::Message& from);
  public:
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const final;
  void InternalSwap(GPUComputeCapability* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "tensorflow.profiler.GPUComputeCapability";
  }
  protected:
  explicit GPUComputeCapability(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  static const ClassData _class_data_;
  const ::PROTOBUF_NAMESPACE_ID::Message::ClassData*GetClassData() const final;

  ::PROTOBUF_NAMESPACE_ID::Metadata GetMetadata() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kMajorFieldNumber = 1,
    kMinorFieldNumber = 2,
  };
  // uint32 major = 1;
  void clear_major();
  uint32_t major() const;
  void set_major(uint32_t value);
  private:
  uint32_t _internal_major() const;
  void _internal_set_major(uint32_t value);
  public:

  // uint32 minor = 2;
  void clear_minor();
  uint32_t minor() const;
  void set_minor(uint32_t value);
  private:
  uint32_t _internal_minor() const;
  void _internal_set_minor(uint32_t value);
  public:

  // @@protoc_insertion_point(class_scope:tensorflow.profiler.GPUComputeCapability)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  uint32_t major_;
  uint32_t minor_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_tensorflow_2fcore_2fprofiler_2fprotobuf_2fhardware_5ftypes_2eproto;
};
// -------------------------------------------------------------------

class DeviceCapabilities final :
    public ::PROTOBUF_NAMESPACE_ID::Message /* @@protoc_insertion_point(class_definition:tensorflow.profiler.DeviceCapabilities) */ {
 public:
  inline DeviceCapabilities() : DeviceCapabilities(nullptr) {}
  ~DeviceCapabilities() override;
  explicit constexpr DeviceCapabilities(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  DeviceCapabilities(const DeviceCapabilities& from);
  DeviceCapabilities(DeviceCapabilities&& from) noexcept
    : DeviceCapabilities() {
    *this = ::std::move(from);
  }

  inline DeviceCapabilities& operator=(const DeviceCapabilities& from) {
    CopyFrom(from);
    return *this;
  }
  inline DeviceCapabilities& operator=(DeviceCapabilities&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const ::PROTOBUF_NAMESPACE_ID::Descriptor* descriptor() {
    return GetDescriptor();
  }
  static const ::PROTOBUF_NAMESPACE_ID::Descriptor* GetDescriptor() {
    return default_instance().GetMetadata().descriptor;
  }
  static const ::PROTOBUF_NAMESPACE_ID::Reflection* GetReflection() {
    return default_instance().GetMetadata().reflection;
  }
  static const DeviceCapabilities& default_instance() {
    return *internal_default_instance();
  }
  static inline const DeviceCapabilities* internal_default_instance() {
    return reinterpret_cast<const DeviceCapabilities*>(
               &_DeviceCapabilities_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    1;

  friend void swap(DeviceCapabilities& a, DeviceCapabilities& b) {
    a.Swap(&b);
  }
  inline void Swap(DeviceCapabilities* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(DeviceCapabilities* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  DeviceCapabilities* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<DeviceCapabilities>(arena);
  }
  using ::PROTOBUF_NAMESPACE_ID::Message::CopyFrom;
  void CopyFrom(const DeviceCapabilities& from);
  using ::PROTOBUF_NAMESPACE_ID::Message::MergeFrom;
  void MergeFrom(const DeviceCapabilities& from);
  private:
  static void MergeImpl(::PROTOBUF_NAMESPACE_ID::Message* to, const ::PROTOBUF_NAMESPACE_ID::Message& from);
  public:
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const final;
  void InternalSwap(DeviceCapabilities* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "tensorflow.profiler.DeviceCapabilities";
  }
  protected:
  explicit DeviceCapabilities(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  static const ClassData _class_data_;
  const ::PROTOBUF_NAMESPACE_ID::Message::ClassData*GetClassData() const final;

  ::PROTOBUF_NAMESPACE_ID::Metadata GetMetadata() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kDeviceVendorFieldNumber = 6,
    kComputeCapabilityFieldNumber = 5,
    kClockRateInGhzFieldNumber = 1,
    kMemorySizeInBytesFieldNumber = 3,
    kMemoryBandwidthFieldNumber = 4,
    kNumCoresFieldNumber = 2,
  };
  // string device_vendor = 6;
  void clear_device_vendor();
  const std::string& device_vendor() const;
  template <typename ArgT0 = const std::string&, typename... ArgT>
  void set_device_vendor(ArgT0&& arg0, ArgT... args);
  std::string* mutable_device_vendor();
  PROTOBUF_NODISCARD std::string* release_device_vendor();
  void set_allocated_device_vendor(std::string* device_vendor);
  private:
  const std::string& _internal_device_vendor() const;
  inline PROTOBUF_ALWAYS_INLINE void _internal_set_device_vendor(const std::string& value);
  std::string* _internal_mutable_device_vendor();
  public:

  // .tensorflow.profiler.GPUComputeCapability compute_capability = 5;
  bool has_compute_capability() const;
  private:
  bool _internal_has_compute_capability() const;
  public:
  void clear_compute_capability();
  const ::tensorflow::profiler::GPUComputeCapability& compute_capability() const;
  PROTOBUF_NODISCARD ::tensorflow::profiler::GPUComputeCapability* release_compute_capability();
  ::tensorflow::profiler::GPUComputeCapability* mutable_compute_capability();
  void set_allocated_compute_capability(::tensorflow::profiler::GPUComputeCapability* compute_capability);
  private:
  const ::tensorflow::profiler::GPUComputeCapability& _internal_compute_capability() const;
  ::tensorflow::profiler::GPUComputeCapability* _internal_mutable_compute_capability();
  public:
  void unsafe_arena_set_allocated_compute_capability(
      ::tensorflow::profiler::GPUComputeCapability* compute_capability);
  ::tensorflow::profiler::GPUComputeCapability* unsafe_arena_release_compute_capability();

  // double clock_rate_in_ghz = 1;
  void clear_clock_rate_in_ghz();
  double clock_rate_in_ghz() const;
  void set_clock_rate_in_ghz(double value);
  private:
  double _internal_clock_rate_in_ghz() const;
  void _internal_set_clock_rate_in_ghz(double value);
  public:

  // uint64 memory_size_in_bytes = 3;
  void clear_memory_size_in_bytes();
  uint64_t memory_size_in_bytes() const;
  void set_memory_size_in_bytes(uint64_t value);
  private:
  uint64_t _internal_memory_size_in_bytes() const;
  void _internal_set_memory_size_in_bytes(uint64_t value);
  public:

  // uint64 memory_bandwidth = 4;
  void clear_memory_bandwidth();
  uint64_t memory_bandwidth() const;
  void set_memory_bandwidth(uint64_t value);
  private:
  uint64_t _internal_memory_bandwidth() const;
  void _internal_set_memory_bandwidth(uint64_t value);
  public:

  // uint32 num_cores = 2;
  void clear_num_cores();
  uint32_t num_cores() const;
  void set_num_cores(uint32_t value);
  private:
  uint32_t _internal_num_cores() const;
  void _internal_set_num_cores(uint32_t value);
  public:

  // @@protoc_insertion_point(class_scope:tensorflow.profiler.DeviceCapabilities)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  ::PROTOBUF_NAMESPACE_ID::internal::ArenaStringPtr device_vendor_;
  ::tensorflow::profiler::GPUComputeCapability* compute_capability_;
  double clock_rate_in_ghz_;
  uint64_t memory_size_in_bytes_;
  uint64_t memory_bandwidth_;
  uint32_t num_cores_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_tensorflow_2fcore_2fprofiler_2fprotobuf_2fhardware_5ftypes_2eproto;
};
// ===================================================================


// ===================================================================

#ifdef __GNUC__
  #pragma GCC diagnostic push
  #pragma GCC diagnostic ignored "-Wstrict-aliasing"
#endif  // __GNUC__
// GPUComputeCapability

// uint32 major = 1;
inline void GPUComputeCapability::clear_major() {
  major_ = 0u;
}
inline uint32_t GPUComputeCapability::_internal_major() const {
  return major_;
}
inline uint32_t GPUComputeCapability::major() const {
  // @@protoc_insertion_point(field_get:tensorflow.profiler.GPUComputeCapability.major)
  return _internal_major();
}
inline void GPUComputeCapability::_internal_set_major(uint32_t value) {
  
  major_ = value;
}
inline void GPUComputeCapability::set_major(uint32_t value) {
  _internal_set_major(value);
  // @@protoc_insertion_point(field_set:tensorflow.profiler.GPUComputeCapability.major)
}

// uint32 minor = 2;
inline void GPUComputeCapability::clear_minor() {
  minor_ = 0u;
}
inline uint32_t GPUComputeCapability::_internal_minor() const {
  return minor_;
}
inline uint32_t GPUComputeCapability::minor() const {
  // @@protoc_insertion_point(field_get:tensorflow.profiler.GPUComputeCapability.minor)
  return _internal_minor();
}
inline void GPUComputeCapability::_internal_set_minor(uint32_t value) {
  
  minor_ = value;
}
inline void GPUComputeCapability::set_minor(uint32_t value) {
  _internal_set_minor(value);
  // @@protoc_insertion_point(field_set:tensorflow.profiler.GPUComputeCapability.minor)
}

// -------------------------------------------------------------------

// DeviceCapabilities

// double clock_rate_in_ghz = 1;
inline void DeviceCapabilities::clear_clock_rate_in_ghz() {
  clock_rate_in_ghz_ = 0;
}
inline double DeviceCapabilities::_internal_clock_rate_in_ghz() const {
  return clock_rate_in_ghz_;
}
inline double DeviceCapabilities::clock_rate_in_ghz() const {
  // @@protoc_insertion_point(field_get:tensorflow.profiler.DeviceCapabilities.clock_rate_in_ghz)
  return _internal_clock_rate_in_ghz();
}
inline void DeviceCapabilities::_internal_set_clock_rate_in_ghz(double value) {
  
  clock_rate_in_ghz_ = value;
}
inline void DeviceCapabilities::set_clock_rate_in_ghz(double value) {
  _internal_set_clock_rate_in_ghz(value);
  // @@protoc_insertion_point(field_set:tensorflow.profiler.DeviceCapabilities.clock_rate_in_ghz)
}

// uint32 num_cores = 2;
inline void DeviceCapabilities::clear_num_cores() {
  num_cores_ = 0u;
}
inline uint32_t DeviceCapabilities::_internal_num_cores() const {
  return num_cores_;
}
inline uint32_t DeviceCapabilities::num_cores() const {
  // @@protoc_insertion_point(field_get:tensorflow.profiler.DeviceCapabilities.num_cores)
  return _internal_num_cores();
}
inline void DeviceCapabilities::_internal_set_num_cores(uint32_t value) {
  
  num_cores_ = value;
}
inline void DeviceCapabilities::set_num_cores(uint32_t value) {
  _internal_set_num_cores(value);
  // @@protoc_insertion_point(field_set:tensorflow.profiler.DeviceCapabilities.num_cores)
}

// uint64 memory_size_in_bytes = 3;
inline void DeviceCapabilities::clear_memory_size_in_bytes() {
  memory_size_in_bytes_ = uint64_t{0u};
}
inline uint64_t DeviceCapabilities::_internal_memory_size_in_bytes() const {
  return memory_size_in_bytes_;
}
inline uint64_t DeviceCapabilities::memory_size_in_bytes() const {
  // @@protoc_insertion_point(field_get:tensorflow.profiler.DeviceCapabilities.memory_size_in_bytes)
  return _internal_memory_size_in_bytes();
}
inline void DeviceCapabilities::_internal_set_memory_size_in_bytes(uint64_t value) {
  
  memory_size_in_bytes_ = value;
}
inline void DeviceCapabilities::set_memory_size_in_bytes(uint64_t value) {
  _internal_set_memory_size_in_bytes(value);
  // @@protoc_insertion_point(field_set:tensorflow.profiler.DeviceCapabilities.memory_size_in_bytes)
}

// uint64 memory_bandwidth = 4;
inline void DeviceCapabilities::clear_memory_bandwidth() {
  memory_bandwidth_ = uint64_t{0u};
}
inline uint64_t DeviceCapabilities::_internal_memory_bandwidth() const {
  return memory_bandwidth_;
}
inline uint64_t DeviceCapabilities::memory_bandwidth() const {
  // @@protoc_insertion_point(field_get:tensorflow.profiler.DeviceCapabilities.memory_bandwidth)
  return _internal_memory_bandwidth();
}
inline void DeviceCapabilities::_internal_set_memory_bandwidth(uint64_t value) {
  
  memory_bandwidth_ = value;
}
inline void DeviceCapabilities::set_memory_bandwidth(uint64_t value) {
  _internal_set_memory_bandwidth(value);
  // @@protoc_insertion_point(field_set:tensorflow.profiler.DeviceCapabilities.memory_bandwidth)
}

// .tensorflow.profiler.GPUComputeCapability compute_capability = 5;
inline bool DeviceCapabilities::_internal_has_compute_capability() const {
  return this != internal_default_instance() && compute_capability_ != nullptr;
}
inline bool DeviceCapabilities::has_compute_capability() const {
  return _internal_has_compute_capability();
}
inline void DeviceCapabilities::clear_compute_capability() {
  if (GetArenaForAllocation() == nullptr && compute_capability_ != nullptr) {
    delete compute_capability_;
  }
  compute_capability_ = nullptr;
}
inline const ::tensorflow::profiler::GPUComputeCapability& DeviceCapabilities::_internal_compute_capability() const {
  const ::tensorflow::profiler::GPUComputeCapability* p = compute_capability_;
  return p != nullptr ? *p : reinterpret_cast<const ::tensorflow::profiler::GPUComputeCapability&>(
      ::tensorflow::profiler::_GPUComputeCapability_default_instance_);
}
inline const ::tensorflow::profiler::GPUComputeCapability& DeviceCapabilities::compute_capability() const {
  // @@protoc_insertion_point(field_get:tensorflow.profiler.DeviceCapabilities.compute_capability)
  return _internal_compute_capability();
}
inline void DeviceCapabilities::unsafe_arena_set_allocated_compute_capability(
    ::tensorflow::profiler::GPUComputeCapability* compute_capability) {
  if (GetArenaForAllocation() == nullptr) {
    delete reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(compute_capability_);
  }
  compute_capability_ = compute_capability;
  if (compute_capability) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:tensorflow.profiler.DeviceCapabilities.compute_capability)
}
inline ::tensorflow::profiler::GPUComputeCapability* DeviceCapabilities::release_compute_capability() {
  
  ::tensorflow::profiler::GPUComputeCapability* temp = compute_capability_;
  compute_capability_ = nullptr;
#ifdef PROTOBUF_FORCE_COPY_IN_RELEASE
  auto* old =  reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(temp);
  temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  if (GetArenaForAllocation() == nullptr) { delete old; }
#else  // PROTOBUF_FORCE_COPY_IN_RELEASE
  if (GetArenaForAllocation() != nullptr) {
    temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  }
#endif  // !PROTOBUF_FORCE_COPY_IN_RELEASE
  return temp;
}
inline ::tensorflow::profiler::GPUComputeCapability* DeviceCapabilities::unsafe_arena_release_compute_capability() {
  // @@protoc_insertion_point(field_release:tensorflow.profiler.DeviceCapabilities.compute_capability)
  
  ::tensorflow::profiler::GPUComputeCapability* temp = compute_capability_;
  compute_capability_ = nullptr;
  return temp;
}
inline ::tensorflow::profiler::GPUComputeCapability* DeviceCapabilities::_internal_mutable_compute_capability() {
  
  if (compute_capability_ == nullptr) {
    auto* p = CreateMaybeMessage<::tensorflow::profiler::GPUComputeCapability>(GetArenaForAllocation());
    compute_capability_ = p;
  }
  return compute_capability_;
}
inline ::tensorflow::profiler::GPUComputeCapability* DeviceCapabilities::mutable_compute_capability() {
  ::tensorflow::profiler::GPUComputeCapability* _msg = _internal_mutable_compute_capability();
  // @@protoc_insertion_point(field_mutable:tensorflow.profiler.DeviceCapabilities.compute_capability)
  return _msg;
}
inline void DeviceCapabilities::set_allocated_compute_capability(::tensorflow::profiler::GPUComputeCapability* compute_capability) {
  ::PROTOBUF_NAMESPACE_ID::Arena* message_arena = GetArenaForAllocation();
  if (message_arena == nullptr) {
    delete compute_capability_;
  }
  if (compute_capability) {
    ::PROTOBUF_NAMESPACE_ID::Arena* submessage_arena =
        ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper<::tensorflow::profiler::GPUComputeCapability>::GetOwningArena(compute_capability);
    if (message_arena != submessage_arena) {
      compute_capability = ::PROTOBUF_NAMESPACE_ID::internal::GetOwnedMessage(
          message_arena, compute_capability, submessage_arena);
    }
    
  } else {
    
  }
  compute_capability_ = compute_capability;
  // @@protoc_insertion_point(field_set_allocated:tensorflow.profiler.DeviceCapabilities.compute_capability)
}

// string device_vendor = 6;
inline void DeviceCapabilities::clear_device_vendor() {
  device_vendor_.ClearToEmpty();
}
inline const std::string& DeviceCapabilities::device_vendor() const {
  // @@protoc_insertion_point(field_get:tensorflow.profiler.DeviceCapabilities.device_vendor)
  return _internal_device_vendor();
}
template <typename ArgT0, typename... ArgT>
inline PROTOBUF_ALWAYS_INLINE
void DeviceCapabilities::set_device_vendor(ArgT0&& arg0, ArgT... args) {
 
 device_vendor_.Set(::PROTOBUF_NAMESPACE_ID::internal::ArenaStringPtr::EmptyDefault{}, static_cast<ArgT0 &&>(arg0), args..., GetArenaForAllocation());
  // @@protoc_insertion_point(field_set:tensorflow.profiler.DeviceCapabilities.device_vendor)
}
inline std::string* DeviceCapabilities::mutable_device_vendor() {
  std::string* _s = _internal_mutable_device_vendor();
  // @@protoc_insertion_point(field_mutable:tensorflow.profiler.DeviceCapabilities.device_vendor)
  return _s;
}
inline const std::string& DeviceCapabilities::_internal_device_vendor() const {
  return device_vendor_.Get();
}
inline void DeviceCapabilities::_internal_set_device_vendor(const std::string& value) {
  
  device_vendor_.Set(::PROTOBUF_NAMESPACE_ID::internal::ArenaStringPtr::EmptyDefault{}, value, GetArenaForAllocation());
}
inline std::string* DeviceCapabilities::_internal_mutable_device_vendor() {
  
  return device_vendor_.Mutable(::PROTOBUF_NAMESPACE_ID::internal::ArenaStringPtr::EmptyDefault{}, GetArenaForAllocation());
}
inline std::string* DeviceCapabilities::release_device_vendor() {
  // @@protoc_insertion_point(field_release:tensorflow.profiler.DeviceCapabilities.device_vendor)
  return device_vendor_.Release(&::PROTOBUF_NAMESPACE_ID::internal::GetEmptyStringAlreadyInited(), GetArenaForAllocation());
}
inline void DeviceCapabilities::set_allocated_device_vendor(std::string* device_vendor) {
  if (device_vendor != nullptr) {
    
  } else {
    
  }
  device_vendor_.SetAllocated(&::PROTOBUF_NAMESPACE_ID::internal::GetEmptyStringAlreadyInited(), device_vendor,
      GetArenaForAllocation());
#ifdef PROTOBUF_FORCE_COPY_DEFAULT_STRING
  if (device_vendor_.IsDefault(&::PROTOBUF_NAMESPACE_ID::internal::GetEmptyStringAlreadyInited())) {
    device_vendor_.Set(&::PROTOBUF_NAMESPACE_ID::internal::GetEmptyStringAlreadyInited(), "", GetArenaForAllocation());
  }
#endif // PROTOBUF_FORCE_COPY_DEFAULT_STRING
  // @@protoc_insertion_point(field_set_allocated:tensorflow.profiler.DeviceCapabilities.device_vendor)
}

#ifdef __GNUC__
  #pragma GCC diagnostic pop
#endif  // __GNUC__
// -------------------------------------------------------------------


// @@protoc_insertion_point(namespace_scope)

}  // namespace profiler
}  // namespace tensorflow

PROTOBUF_NAMESPACE_OPEN

template <> struct is_proto_enum< ::tensorflow::profiler::HardwareType> : ::std::true_type {};
template <>
inline const EnumDescriptor* GetEnumDescriptor< ::tensorflow::profiler::HardwareType>() {
  return ::tensorflow::profiler::HardwareType_descriptor();
}

PROTOBUF_NAMESPACE_CLOSE

// @@protoc_insertion_point(global_scope)

#include <google/protobuf/port_undef.inc>
#endif  // GOOGLE_PROTOBUF_INCLUDED_GOOGLE_PROTOBUF_INCLUDED_tensorflow_2fcore_2fprofiler_2fprotobuf_2fhardware_5ftypes_2eproto
