// Generated by the protocol buffer compiler.  DO NOT EDIT!
// source: tensorflow/compiler/xla/service/gpu/backend_configs.proto

#ifndef GOOGLE_PROTOBUF_INCLUDED_tensorflow_2fcompiler_2fxla_2fservice_2fgpu_2fbackend_5fconfigs_2eproto
#define GOOGLE_PROTOBUF_INCLUDED_tensorflow_2fcompiler_2fxla_2fservice_2fgpu_2fbackend_5fconfigs_2eproto

#include <limits>
#include <string>

#include <google/protobuf/port_def.inc>
#if PROTOBUF_VERSION < 3019000
#error This file was generated by a newer version of protoc which is
#error incompatible with your Protocol Buffer headers. Please update
#error your headers.
#endif
#if 3019004 < PROTOBUF_MIN_PROTOC_VERSION
#error This file was generated by an older version of protoc which is
#error incompatible with your Protocol Buffer headers. Please
#error regenerate this file with a newer version of protoc.
#endif

#include <google/protobuf/port_undef.inc>
#include <google/protobuf/io/coded_stream.h>
#include <google/protobuf/arena.h>
#include <google/protobuf/arenastring.h>
#include <google/protobuf/generated_message_table_driven.h>
#include <google/protobuf/generated_message_util.h>
#include <google/protobuf/metadata_lite.h>
#include <google/protobuf/generated_message_reflection.h>
#include <google/protobuf/message.h>
#include <google/protobuf/repeated_field.h>  // IWYU pragma: export
#include <google/protobuf/extension_set.h>  // IWYU pragma: export
#include <google/protobuf/unknown_field_set.h>
#include "tensorflow/compiler/xla/xla_data.pb.h"
#include "tensorflow/stream_executor/dnn.pb.h"
// @@protoc_insertion_point(includes)
#include <google/protobuf/port_def.inc>
#define PROTOBUF_INTERNAL_EXPORT_tensorflow_2fcompiler_2fxla_2fservice_2fgpu_2fbackend_5fconfigs_2eproto
PROTOBUF_NAMESPACE_OPEN
namespace internal {
class AnyMetadata;
}  // namespace internal
PROTOBUF_NAMESPACE_CLOSE

// Internal implementation detail -- do not use these members.
struct TableStruct_tensorflow_2fcompiler_2fxla_2fservice_2fgpu_2fbackend_5fconfigs_2eproto {
  static const ::PROTOBUF_NAMESPACE_ID::internal::ParseTableField entries[]
    PROTOBUF_SECTION_VARIABLE(protodesc_cold);
  static const ::PROTOBUF_NAMESPACE_ID::internal::AuxiliaryParseTableField aux[]
    PROTOBUF_SECTION_VARIABLE(protodesc_cold);
  static const ::PROTOBUF_NAMESPACE_ID::internal::ParseTable schema[3]
    PROTOBUF_SECTION_VARIABLE(protodesc_cold);
  static const ::PROTOBUF_NAMESPACE_ID::internal::FieldMetadata field_metadata[];
  static const ::PROTOBUF_NAMESPACE_ID::internal::SerializationTable serialization_table[];
  static const uint32_t offsets[];
};
extern const ::PROTOBUF_NAMESPACE_ID::internal::DescriptorTable descriptor_table_tensorflow_2fcompiler_2fxla_2fservice_2fgpu_2fbackend_5fconfigs_2eproto;
namespace xla {
namespace gpu {
class BitcastBackendConfig;
struct BitcastBackendConfigDefaultTypeInternal;
extern BitcastBackendConfigDefaultTypeInternal _BitcastBackendConfig_default_instance_;
class CudnnConvBackendConfig;
struct CudnnConvBackendConfigDefaultTypeInternal;
extern CudnnConvBackendConfigDefaultTypeInternal _CudnnConvBackendConfig_default_instance_;
class GemmBackendConfig;
struct GemmBackendConfigDefaultTypeInternal;
extern GemmBackendConfigDefaultTypeInternal _GemmBackendConfig_default_instance_;
}  // namespace gpu
}  // namespace xla
PROTOBUF_NAMESPACE_OPEN
template<> ::xla::gpu::BitcastBackendConfig* Arena::CreateMaybeMessage<::xla::gpu::BitcastBackendConfig>(Arena*);
template<> ::xla::gpu::CudnnConvBackendConfig* Arena::CreateMaybeMessage<::xla::gpu::CudnnConvBackendConfig>(Arena*);
template<> ::xla::gpu::GemmBackendConfig* Arena::CreateMaybeMessage<::xla::gpu::GemmBackendConfig>(Arena*);
PROTOBUF_NAMESPACE_CLOSE
namespace xla {
namespace gpu {

// ===================================================================

class CudnnConvBackendConfig final :
    public ::PROTOBUF_NAMESPACE_ID::Message /* @@protoc_insertion_point(class_definition:xla.gpu.CudnnConvBackendConfig) */ {
 public:
  inline CudnnConvBackendConfig() : CudnnConvBackendConfig(nullptr) {}
  ~CudnnConvBackendConfig() override;
  explicit constexpr CudnnConvBackendConfig(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  CudnnConvBackendConfig(const CudnnConvBackendConfig& from);
  CudnnConvBackendConfig(CudnnConvBackendConfig&& from) noexcept
    : CudnnConvBackendConfig() {
    *this = ::std::move(from);
  }

  inline CudnnConvBackendConfig& operator=(const CudnnConvBackendConfig& from) {
    CopyFrom(from);
    return *this;
  }
  inline CudnnConvBackendConfig& operator=(CudnnConvBackendConfig&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const ::PROTOBUF_NAMESPACE_ID::Descriptor* descriptor() {
    return GetDescriptor();
  }
  static const ::PROTOBUF_NAMESPACE_ID::Descriptor* GetDescriptor() {
    return default_instance().GetMetadata().descriptor;
  }
  static const ::PROTOBUF_NAMESPACE_ID::Reflection* GetReflection() {
    return default_instance().GetMetadata().reflection;
  }
  static const CudnnConvBackendConfig& default_instance() {
    return *internal_default_instance();
  }
  static inline const CudnnConvBackendConfig* internal_default_instance() {
    return reinterpret_cast<const CudnnConvBackendConfig*>(
               &_CudnnConvBackendConfig_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    0;

  friend void swap(CudnnConvBackendConfig& a, CudnnConvBackendConfig& b) {
    a.Swap(&b);
  }
  inline void Swap(CudnnConvBackendConfig* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(CudnnConvBackendConfig* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  CudnnConvBackendConfig* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<CudnnConvBackendConfig>(arena);
  }
  using ::PROTOBUF_NAMESPACE_ID::Message::CopyFrom;
  void CopyFrom(const CudnnConvBackendConfig& from);
  using ::PROTOBUF_NAMESPACE_ID::Message::MergeFrom;
  void MergeFrom(const CudnnConvBackendConfig& from);
  private:
  static void MergeImpl(::PROTOBUF_NAMESPACE_ID::Message* to, const ::PROTOBUF_NAMESPACE_ID::Message& from);
  public:
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const final;
  void InternalSwap(CudnnConvBackendConfig* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "xla.gpu.CudnnConvBackendConfig";
  }
  protected:
  explicit CudnnConvBackendConfig(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  static const ClassData _class_data_;
  const ::PROTOBUF_NAMESPACE_ID::Message::ClassData*GetClassData() const final;

  ::PROTOBUF_NAMESPACE_ID::Metadata GetMetadata() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kAlgorithmFieldNumber = 6,
    kActivationModeFieldNumber = 3,
    kConvResultScaleFieldNumber = 4,
    kSideInputScaleFieldNumber = 5,
  };
  // .stream_executor.dnn.AlgorithmProto algorithm = 6;
  bool has_algorithm() const;
  private:
  bool _internal_has_algorithm() const;
  public:
  void clear_algorithm();
  const ::stream_executor::dnn::AlgorithmProto& algorithm() const;
  PROTOBUF_NODISCARD ::stream_executor::dnn::AlgorithmProto* release_algorithm();
  ::stream_executor::dnn::AlgorithmProto* mutable_algorithm();
  void set_allocated_algorithm(::stream_executor::dnn::AlgorithmProto* algorithm);
  private:
  const ::stream_executor::dnn::AlgorithmProto& _internal_algorithm() const;
  ::stream_executor::dnn::AlgorithmProto* _internal_mutable_algorithm();
  public:
  void unsafe_arena_set_allocated_algorithm(
      ::stream_executor::dnn::AlgorithmProto* algorithm);
  ::stream_executor::dnn::AlgorithmProto* unsafe_arena_release_algorithm();

  // int64 activation_mode = 3;
  void clear_activation_mode();
  int64_t activation_mode() const;
  void set_activation_mode(int64_t value);
  private:
  int64_t _internal_activation_mode() const;
  void _internal_set_activation_mode(int64_t value);
  public:

  // double conv_result_scale = 4;
  void clear_conv_result_scale();
  double conv_result_scale() const;
  void set_conv_result_scale(double value);
  private:
  double _internal_conv_result_scale() const;
  void _internal_set_conv_result_scale(double value);
  public:

  // double side_input_scale = 5;
  void clear_side_input_scale();
  double side_input_scale() const;
  void set_side_input_scale(double value);
  private:
  double _internal_side_input_scale() const;
  void _internal_set_side_input_scale(double value);
  public:

  // @@protoc_insertion_point(class_scope:xla.gpu.CudnnConvBackendConfig)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  ::stream_executor::dnn::AlgorithmProto* algorithm_;
  int64_t activation_mode_;
  double conv_result_scale_;
  double side_input_scale_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_tensorflow_2fcompiler_2fxla_2fservice_2fgpu_2fbackend_5fconfigs_2eproto;
};
// -------------------------------------------------------------------

class GemmBackendConfig final :
    public ::PROTOBUF_NAMESPACE_ID::Message /* @@protoc_insertion_point(class_definition:xla.gpu.GemmBackendConfig) */ {
 public:
  inline GemmBackendConfig() : GemmBackendConfig(nullptr) {}
  ~GemmBackendConfig() override;
  explicit constexpr GemmBackendConfig(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  GemmBackendConfig(const GemmBackendConfig& from);
  GemmBackendConfig(GemmBackendConfig&& from) noexcept
    : GemmBackendConfig() {
    *this = ::std::move(from);
  }

  inline GemmBackendConfig& operator=(const GemmBackendConfig& from) {
    CopyFrom(from);
    return *this;
  }
  inline GemmBackendConfig& operator=(GemmBackendConfig&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const ::PROTOBUF_NAMESPACE_ID::Descriptor* descriptor() {
    return GetDescriptor();
  }
  static const ::PROTOBUF_NAMESPACE_ID::Descriptor* GetDescriptor() {
    return default_instance().GetMetadata().descriptor;
  }
  static const ::PROTOBUF_NAMESPACE_ID::Reflection* GetReflection() {
    return default_instance().GetMetadata().reflection;
  }
  static const GemmBackendConfig& default_instance() {
    return *internal_default_instance();
  }
  enum AlgorithmCase {
    kSelectedAlgorithm = 1,
    ALGORITHM_NOT_SET = 0,
  };

  static inline const GemmBackendConfig* internal_default_instance() {
    return reinterpret_cast<const GemmBackendConfig*>(
               &_GemmBackendConfig_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    1;

  friend void swap(GemmBackendConfig& a, GemmBackendConfig& b) {
    a.Swap(&b);
  }
  inline void Swap(GemmBackendConfig* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(GemmBackendConfig* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  GemmBackendConfig* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<GemmBackendConfig>(arena);
  }
  using ::PROTOBUF_NAMESPACE_ID::Message::CopyFrom;
  void CopyFrom(const GemmBackendConfig& from);
  using ::PROTOBUF_NAMESPACE_ID::Message::MergeFrom;
  void MergeFrom(const GemmBackendConfig& from);
  private:
  static void MergeImpl(::PROTOBUF_NAMESPACE_ID::Message* to, const ::PROTOBUF_NAMESPACE_ID::Message& from);
  public:
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const final;
  void InternalSwap(GemmBackendConfig* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "xla.gpu.GemmBackendConfig";
  }
  protected:
  explicit GemmBackendConfig(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  static const ClassData _class_data_;
  const ::PROTOBUF_NAMESPACE_ID::Message::ClassData*GetClassData() const final;

  ::PROTOBUF_NAMESPACE_ID::Metadata GetMetadata() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kDotDimensionNumbersFieldNumber = 7,
    kAlphaRealFieldNumber = 2,
    kBetaFieldNumber = 3,
    kBatchSizeFieldNumber = 8,
    kAlphaImagFieldNumber = 9,
    kLhsStrideFieldNumber = 10,
    kRhsStrideFieldNumber = 11,
    kSelectedAlgorithmFieldNumber = 1,
  };
  // .xla.DotDimensionNumbers dot_dimension_numbers = 7;
  bool has_dot_dimension_numbers() const;
  private:
  bool _internal_has_dot_dimension_numbers() const;
  public:
  void clear_dot_dimension_numbers();
  const ::xla::DotDimensionNumbers& dot_dimension_numbers() const;
  PROTOBUF_NODISCARD ::xla::DotDimensionNumbers* release_dot_dimension_numbers();
  ::xla::DotDimensionNumbers* mutable_dot_dimension_numbers();
  void set_allocated_dot_dimension_numbers(::xla::DotDimensionNumbers* dot_dimension_numbers);
  private:
  const ::xla::DotDimensionNumbers& _internal_dot_dimension_numbers() const;
  ::xla::DotDimensionNumbers* _internal_mutable_dot_dimension_numbers();
  public:
  void unsafe_arena_set_allocated_dot_dimension_numbers(
      ::xla::DotDimensionNumbers* dot_dimension_numbers);
  ::xla::DotDimensionNumbers* unsafe_arena_release_dot_dimension_numbers();

  // double alpha_real = 2;
  void clear_alpha_real();
  double alpha_real() const;
  void set_alpha_real(double value);
  private:
  double _internal_alpha_real() const;
  void _internal_set_alpha_real(double value);
  public:

  // double beta = 3;
  void clear_beta();
  double beta() const;
  void set_beta(double value);
  private:
  double _internal_beta() const;
  void _internal_set_beta(double value);
  public:

  // int64 batch_size = 8;
  void clear_batch_size();
  int64_t batch_size() const;
  void set_batch_size(int64_t value);
  private:
  int64_t _internal_batch_size() const;
  void _internal_set_batch_size(int64_t value);
  public:

  // double alpha_imag = 9;
  void clear_alpha_imag();
  double alpha_imag() const;
  void set_alpha_imag(double value);
  private:
  double _internal_alpha_imag() const;
  void _internal_set_alpha_imag(double value);
  public:

  // int64 lhs_stride = 10;
  void clear_lhs_stride();
  int64_t lhs_stride() const;
  void set_lhs_stride(int64_t value);
  private:
  int64_t _internal_lhs_stride() const;
  void _internal_set_lhs_stride(int64_t value);
  public:

  // int64 rhs_stride = 11;
  void clear_rhs_stride();
  int64_t rhs_stride() const;
  void set_rhs_stride(int64_t value);
  private:
  int64_t _internal_rhs_stride() const;
  void _internal_set_rhs_stride(int64_t value);
  public:

  // int64 selected_algorithm = 1;
  bool has_selected_algorithm() const;
  private:
  bool _internal_has_selected_algorithm() const;
  public:
  void clear_selected_algorithm();
  int64_t selected_algorithm() const;
  void set_selected_algorithm(int64_t value);
  private:
  int64_t _internal_selected_algorithm() const;
  void _internal_set_selected_algorithm(int64_t value);
  public:

  void clear_algorithm();
  AlgorithmCase algorithm_case() const;
  // @@protoc_insertion_point(class_scope:xla.gpu.GemmBackendConfig)
 private:
  class _Internal;
  void set_has_selected_algorithm();

  inline bool has_algorithm() const;
  inline void clear_has_algorithm();

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  ::xla::DotDimensionNumbers* dot_dimension_numbers_;
  double alpha_real_;
  double beta_;
  int64_t batch_size_;
  double alpha_imag_;
  int64_t lhs_stride_;
  int64_t rhs_stride_;
  union AlgorithmUnion {
    constexpr AlgorithmUnion() : _constinit_{} {}
      ::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized _constinit_;
    int64_t selected_algorithm_;
  } algorithm_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  uint32_t _oneof_case_[1];

  friend struct ::TableStruct_tensorflow_2fcompiler_2fxla_2fservice_2fgpu_2fbackend_5fconfigs_2eproto;
};
// -------------------------------------------------------------------

class BitcastBackendConfig final :
    public ::PROTOBUF_NAMESPACE_ID::Message /* @@protoc_insertion_point(class_definition:xla.gpu.BitcastBackendConfig) */ {
 public:
  inline BitcastBackendConfig() : BitcastBackendConfig(nullptr) {}
  ~BitcastBackendConfig() override;
  explicit constexpr BitcastBackendConfig(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  BitcastBackendConfig(const BitcastBackendConfig& from);
  BitcastBackendConfig(BitcastBackendConfig&& from) noexcept
    : BitcastBackendConfig() {
    *this = ::std::move(from);
  }

  inline BitcastBackendConfig& operator=(const BitcastBackendConfig& from) {
    CopyFrom(from);
    return *this;
  }
  inline BitcastBackendConfig& operator=(BitcastBackendConfig&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const ::PROTOBUF_NAMESPACE_ID::Descriptor* descriptor() {
    return GetDescriptor();
  }
  static const ::PROTOBUF_NAMESPACE_ID::Descriptor* GetDescriptor() {
    return default_instance().GetMetadata().descriptor;
  }
  static const ::PROTOBUF_NAMESPACE_ID::Reflection* GetReflection() {
    return default_instance().GetMetadata().reflection;
  }
  static const BitcastBackendConfig& default_instance() {
    return *internal_default_instance();
  }
  static inline const BitcastBackendConfig* internal_default_instance() {
    return reinterpret_cast<const BitcastBackendConfig*>(
               &_BitcastBackendConfig_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    2;

  friend void swap(BitcastBackendConfig& a, BitcastBackendConfig& b) {
    a.Swap(&b);
  }
  inline void Swap(BitcastBackendConfig* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(BitcastBackendConfig* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  BitcastBackendConfig* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<BitcastBackendConfig>(arena);
  }
  using ::PROTOBUF_NAMESPACE_ID::Message::CopyFrom;
  void CopyFrom(const BitcastBackendConfig& from);
  using ::PROTOBUF_NAMESPACE_ID::Message::MergeFrom;
  void MergeFrom(const BitcastBackendConfig& from);
  private:
  static void MergeImpl(::PROTOBUF_NAMESPACE_ID::Message* to, const ::PROTOBUF_NAMESPACE_ID::Message& from);
  public:
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const final;
  void InternalSwap(BitcastBackendConfig* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "xla.gpu.BitcastBackendConfig";
  }
  protected:
  explicit BitcastBackendConfig(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  static const ClassData _class_data_;
  const ::PROTOBUF_NAMESPACE_ID::Message::ClassData*GetClassData() const final;

  ::PROTOBUF_NAMESPACE_ID::Metadata GetMetadata() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kSourceLayoutFieldNumber = 1,
    kResultLayoutFieldNumber = 2,
  };
  // .xla.LayoutProto source_layout = 1;
  bool has_source_layout() const;
  private:
  bool _internal_has_source_layout() const;
  public:
  void clear_source_layout();
  const ::xla::LayoutProto& source_layout() const;
  PROTOBUF_NODISCARD ::xla::LayoutProto* release_source_layout();
  ::xla::LayoutProto* mutable_source_layout();
  void set_allocated_source_layout(::xla::LayoutProto* source_layout);
  private:
  const ::xla::LayoutProto& _internal_source_layout() const;
  ::xla::LayoutProto* _internal_mutable_source_layout();
  public:
  void unsafe_arena_set_allocated_source_layout(
      ::xla::LayoutProto* source_layout);
  ::xla::LayoutProto* unsafe_arena_release_source_layout();

  // .xla.LayoutProto result_layout = 2;
  bool has_result_layout() const;
  private:
  bool _internal_has_result_layout() const;
  public:
  void clear_result_layout();
  const ::xla::LayoutProto& result_layout() const;
  PROTOBUF_NODISCARD ::xla::LayoutProto* release_result_layout();
  ::xla::LayoutProto* mutable_result_layout();
  void set_allocated_result_layout(::xla::LayoutProto* result_layout);
  private:
  const ::xla::LayoutProto& _internal_result_layout() const;
  ::xla::LayoutProto* _internal_mutable_result_layout();
  public:
  void unsafe_arena_set_allocated_result_layout(
      ::xla::LayoutProto* result_layout);
  ::xla::LayoutProto* unsafe_arena_release_result_layout();

  // @@protoc_insertion_point(class_scope:xla.gpu.BitcastBackendConfig)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  ::xla::LayoutProto* source_layout_;
  ::xla::LayoutProto* result_layout_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_tensorflow_2fcompiler_2fxla_2fservice_2fgpu_2fbackend_5fconfigs_2eproto;
};
// ===================================================================


// ===================================================================

#ifdef __GNUC__
  #pragma GCC diagnostic push
  #pragma GCC diagnostic ignored "-Wstrict-aliasing"
#endif  // __GNUC__
// CudnnConvBackendConfig

// .stream_executor.dnn.AlgorithmProto algorithm = 6;
inline bool CudnnConvBackendConfig::_internal_has_algorithm() const {
  return this != internal_default_instance() && algorithm_ != nullptr;
}
inline bool CudnnConvBackendConfig::has_algorithm() const {
  return _internal_has_algorithm();
}
inline const ::stream_executor::dnn::AlgorithmProto& CudnnConvBackendConfig::_internal_algorithm() const {
  const ::stream_executor::dnn::AlgorithmProto* p = algorithm_;
  return p != nullptr ? *p : reinterpret_cast<const ::stream_executor::dnn::AlgorithmProto&>(
      ::stream_executor::dnn::_AlgorithmProto_default_instance_);
}
inline const ::stream_executor::dnn::AlgorithmProto& CudnnConvBackendConfig::algorithm() const {
  // @@protoc_insertion_point(field_get:xla.gpu.CudnnConvBackendConfig.algorithm)
  return _internal_algorithm();
}
inline void CudnnConvBackendConfig::unsafe_arena_set_allocated_algorithm(
    ::stream_executor::dnn::AlgorithmProto* algorithm) {
  if (GetArenaForAllocation() == nullptr) {
    delete reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(algorithm_);
  }
  algorithm_ = algorithm;
  if (algorithm) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:xla.gpu.CudnnConvBackendConfig.algorithm)
}
inline ::stream_executor::dnn::AlgorithmProto* CudnnConvBackendConfig::release_algorithm() {
  
  ::stream_executor::dnn::AlgorithmProto* temp = algorithm_;
  algorithm_ = nullptr;
#ifdef PROTOBUF_FORCE_COPY_IN_RELEASE
  auto* old =  reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(temp);
  temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  if (GetArenaForAllocation() == nullptr) { delete old; }
#else  // PROTOBUF_FORCE_COPY_IN_RELEASE
  if (GetArenaForAllocation() != nullptr) {
    temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  }
#endif  // !PROTOBUF_FORCE_COPY_IN_RELEASE
  return temp;
}
inline ::stream_executor::dnn::AlgorithmProto* CudnnConvBackendConfig::unsafe_arena_release_algorithm() {
  // @@protoc_insertion_point(field_release:xla.gpu.CudnnConvBackendConfig.algorithm)
  
  ::stream_executor::dnn::AlgorithmProto* temp = algorithm_;
  algorithm_ = nullptr;
  return temp;
}
inline ::stream_executor::dnn::AlgorithmProto* CudnnConvBackendConfig::_internal_mutable_algorithm() {
  
  if (algorithm_ == nullptr) {
    auto* p = CreateMaybeMessage<::stream_executor::dnn::AlgorithmProto>(GetArenaForAllocation());
    algorithm_ = p;
  }
  return algorithm_;
}
inline ::stream_executor::dnn::AlgorithmProto* CudnnConvBackendConfig::mutable_algorithm() {
  ::stream_executor::dnn::AlgorithmProto* _msg = _internal_mutable_algorithm();
  // @@protoc_insertion_point(field_mutable:xla.gpu.CudnnConvBackendConfig.algorithm)
  return _msg;
}
inline void CudnnConvBackendConfig::set_allocated_algorithm(::stream_executor::dnn::AlgorithmProto* algorithm) {
  ::PROTOBUF_NAMESPACE_ID::Arena* message_arena = GetArenaForAllocation();
  if (message_arena == nullptr) {
    delete reinterpret_cast< ::PROTOBUF_NAMESPACE_ID::MessageLite*>(algorithm_);
  }
  if (algorithm) {
    ::PROTOBUF_NAMESPACE_ID::Arena* submessage_arena =
        ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper<
            ::PROTOBUF_NAMESPACE_ID::MessageLite>::GetOwningArena(
                reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(algorithm));
    if (message_arena != submessage_arena) {
      algorithm = ::PROTOBUF_NAMESPACE_ID::internal::GetOwnedMessage(
          message_arena, algorithm, submessage_arena);
    }
    
  } else {
    
  }
  algorithm_ = algorithm;
  // @@protoc_insertion_point(field_set_allocated:xla.gpu.CudnnConvBackendConfig.algorithm)
}

// double conv_result_scale = 4;
inline void CudnnConvBackendConfig::clear_conv_result_scale() {
  conv_result_scale_ = 0;
}
inline double CudnnConvBackendConfig::_internal_conv_result_scale() const {
  return conv_result_scale_;
}
inline double CudnnConvBackendConfig::conv_result_scale() const {
  // @@protoc_insertion_point(field_get:xla.gpu.CudnnConvBackendConfig.conv_result_scale)
  return _internal_conv_result_scale();
}
inline void CudnnConvBackendConfig::_internal_set_conv_result_scale(double value) {
  
  conv_result_scale_ = value;
}
inline void CudnnConvBackendConfig::set_conv_result_scale(double value) {
  _internal_set_conv_result_scale(value);
  // @@protoc_insertion_point(field_set:xla.gpu.CudnnConvBackendConfig.conv_result_scale)
}

// int64 activation_mode = 3;
inline void CudnnConvBackendConfig::clear_activation_mode() {
  activation_mode_ = int64_t{0};
}
inline int64_t CudnnConvBackendConfig::_internal_activation_mode() const {
  return activation_mode_;
}
inline int64_t CudnnConvBackendConfig::activation_mode() const {
  // @@protoc_insertion_point(field_get:xla.gpu.CudnnConvBackendConfig.activation_mode)
  return _internal_activation_mode();
}
inline void CudnnConvBackendConfig::_internal_set_activation_mode(int64_t value) {
  
  activation_mode_ = value;
}
inline void CudnnConvBackendConfig::set_activation_mode(int64_t value) {
  _internal_set_activation_mode(value);
  // @@protoc_insertion_point(field_set:xla.gpu.CudnnConvBackendConfig.activation_mode)
}

// double side_input_scale = 5;
inline void CudnnConvBackendConfig::clear_side_input_scale() {
  side_input_scale_ = 0;
}
inline double CudnnConvBackendConfig::_internal_side_input_scale() const {
  return side_input_scale_;
}
inline double CudnnConvBackendConfig::side_input_scale() const {
  // @@protoc_insertion_point(field_get:xla.gpu.CudnnConvBackendConfig.side_input_scale)
  return _internal_side_input_scale();
}
inline void CudnnConvBackendConfig::_internal_set_side_input_scale(double value) {
  
  side_input_scale_ = value;
}
inline void CudnnConvBackendConfig::set_side_input_scale(double value) {
  _internal_set_side_input_scale(value);
  // @@protoc_insertion_point(field_set:xla.gpu.CudnnConvBackendConfig.side_input_scale)
}

// -------------------------------------------------------------------

// GemmBackendConfig

// int64 selected_algorithm = 1;
inline bool GemmBackendConfig::_internal_has_selected_algorithm() const {
  return algorithm_case() == kSelectedAlgorithm;
}
inline bool GemmBackendConfig::has_selected_algorithm() const {
  return _internal_has_selected_algorithm();
}
inline void GemmBackendConfig::set_has_selected_algorithm() {
  _oneof_case_[0] = kSelectedAlgorithm;
}
inline void GemmBackendConfig::clear_selected_algorithm() {
  if (_internal_has_selected_algorithm()) {
    algorithm_.selected_algorithm_ = int64_t{0};
    clear_has_algorithm();
  }
}
inline int64_t GemmBackendConfig::_internal_selected_algorithm() const {
  if (_internal_has_selected_algorithm()) {
    return algorithm_.selected_algorithm_;
  }
  return int64_t{0};
}
inline void GemmBackendConfig::_internal_set_selected_algorithm(int64_t value) {
  if (!_internal_has_selected_algorithm()) {
    clear_algorithm();
    set_has_selected_algorithm();
  }
  algorithm_.selected_algorithm_ = value;
}
inline int64_t GemmBackendConfig::selected_algorithm() const {
  // @@protoc_insertion_point(field_get:xla.gpu.GemmBackendConfig.selected_algorithm)
  return _internal_selected_algorithm();
}
inline void GemmBackendConfig::set_selected_algorithm(int64_t value) {
  _internal_set_selected_algorithm(value);
  // @@protoc_insertion_point(field_set:xla.gpu.GemmBackendConfig.selected_algorithm)
}

// double alpha_real = 2;
inline void GemmBackendConfig::clear_alpha_real() {
  alpha_real_ = 0;
}
inline double GemmBackendConfig::_internal_alpha_real() const {
  return alpha_real_;
}
inline double GemmBackendConfig::alpha_real() const {
  // @@protoc_insertion_point(field_get:xla.gpu.GemmBackendConfig.alpha_real)
  return _internal_alpha_real();
}
inline void GemmBackendConfig::_internal_set_alpha_real(double value) {
  
  alpha_real_ = value;
}
inline void GemmBackendConfig::set_alpha_real(double value) {
  _internal_set_alpha_real(value);
  // @@protoc_insertion_point(field_set:xla.gpu.GemmBackendConfig.alpha_real)
}

// double alpha_imag = 9;
inline void GemmBackendConfig::clear_alpha_imag() {
  alpha_imag_ = 0;
}
inline double GemmBackendConfig::_internal_alpha_imag() const {
  return alpha_imag_;
}
inline double GemmBackendConfig::alpha_imag() const {
  // @@protoc_insertion_point(field_get:xla.gpu.GemmBackendConfig.alpha_imag)
  return _internal_alpha_imag();
}
inline void GemmBackendConfig::_internal_set_alpha_imag(double value) {
  
  alpha_imag_ = value;
}
inline void GemmBackendConfig::set_alpha_imag(double value) {
  _internal_set_alpha_imag(value);
  // @@protoc_insertion_point(field_set:xla.gpu.GemmBackendConfig.alpha_imag)
}

// double beta = 3;
inline void GemmBackendConfig::clear_beta() {
  beta_ = 0;
}
inline double GemmBackendConfig::_internal_beta() const {
  return beta_;
}
inline double GemmBackendConfig::beta() const {
  // @@protoc_insertion_point(field_get:xla.gpu.GemmBackendConfig.beta)
  return _internal_beta();
}
inline void GemmBackendConfig::_internal_set_beta(double value) {
  
  beta_ = value;
}
inline void GemmBackendConfig::set_beta(double value) {
  _internal_set_beta(value);
  // @@protoc_insertion_point(field_set:xla.gpu.GemmBackendConfig.beta)
}

// .xla.DotDimensionNumbers dot_dimension_numbers = 7;
inline bool GemmBackendConfig::_internal_has_dot_dimension_numbers() const {
  return this != internal_default_instance() && dot_dimension_numbers_ != nullptr;
}
inline bool GemmBackendConfig::has_dot_dimension_numbers() const {
  return _internal_has_dot_dimension_numbers();
}
inline const ::xla::DotDimensionNumbers& GemmBackendConfig::_internal_dot_dimension_numbers() const {
  const ::xla::DotDimensionNumbers* p = dot_dimension_numbers_;
  return p != nullptr ? *p : reinterpret_cast<const ::xla::DotDimensionNumbers&>(
      ::xla::_DotDimensionNumbers_default_instance_);
}
inline const ::xla::DotDimensionNumbers& GemmBackendConfig::dot_dimension_numbers() const {
  // @@protoc_insertion_point(field_get:xla.gpu.GemmBackendConfig.dot_dimension_numbers)
  return _internal_dot_dimension_numbers();
}
inline void GemmBackendConfig::unsafe_arena_set_allocated_dot_dimension_numbers(
    ::xla::DotDimensionNumbers* dot_dimension_numbers) {
  if (GetArenaForAllocation() == nullptr) {
    delete reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(dot_dimension_numbers_);
  }
  dot_dimension_numbers_ = dot_dimension_numbers;
  if (dot_dimension_numbers) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:xla.gpu.GemmBackendConfig.dot_dimension_numbers)
}
inline ::xla::DotDimensionNumbers* GemmBackendConfig::release_dot_dimension_numbers() {
  
  ::xla::DotDimensionNumbers* temp = dot_dimension_numbers_;
  dot_dimension_numbers_ = nullptr;
#ifdef PROTOBUF_FORCE_COPY_IN_RELEASE
  auto* old =  reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(temp);
  temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  if (GetArenaForAllocation() == nullptr) { delete old; }
#else  // PROTOBUF_FORCE_COPY_IN_RELEASE
  if (GetArenaForAllocation() != nullptr) {
    temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  }
#endif  // !PROTOBUF_FORCE_COPY_IN_RELEASE
  return temp;
}
inline ::xla::DotDimensionNumbers* GemmBackendConfig::unsafe_arena_release_dot_dimension_numbers() {
  // @@protoc_insertion_point(field_release:xla.gpu.GemmBackendConfig.dot_dimension_numbers)
  
  ::xla::DotDimensionNumbers* temp = dot_dimension_numbers_;
  dot_dimension_numbers_ = nullptr;
  return temp;
}
inline ::xla::DotDimensionNumbers* GemmBackendConfig::_internal_mutable_dot_dimension_numbers() {
  
  if (dot_dimension_numbers_ == nullptr) {
    auto* p = CreateMaybeMessage<::xla::DotDimensionNumbers>(GetArenaForAllocation());
    dot_dimension_numbers_ = p;
  }
  return dot_dimension_numbers_;
}
inline ::xla::DotDimensionNumbers* GemmBackendConfig::mutable_dot_dimension_numbers() {
  ::xla::DotDimensionNumbers* _msg = _internal_mutable_dot_dimension_numbers();
  // @@protoc_insertion_point(field_mutable:xla.gpu.GemmBackendConfig.dot_dimension_numbers)
  return _msg;
}
inline void GemmBackendConfig::set_allocated_dot_dimension_numbers(::xla::DotDimensionNumbers* dot_dimension_numbers) {
  ::PROTOBUF_NAMESPACE_ID::Arena* message_arena = GetArenaForAllocation();
  if (message_arena == nullptr) {
    delete reinterpret_cast< ::PROTOBUF_NAMESPACE_ID::MessageLite*>(dot_dimension_numbers_);
  }
  if (dot_dimension_numbers) {
    ::PROTOBUF_NAMESPACE_ID::Arena* submessage_arena =
        ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper<
            ::PROTOBUF_NAMESPACE_ID::MessageLite>::GetOwningArena(
                reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(dot_dimension_numbers));
    if (message_arena != submessage_arena) {
      dot_dimension_numbers = ::PROTOBUF_NAMESPACE_ID::internal::GetOwnedMessage(
          message_arena, dot_dimension_numbers, submessage_arena);
    }
    
  } else {
    
  }
  dot_dimension_numbers_ = dot_dimension_numbers;
  // @@protoc_insertion_point(field_set_allocated:xla.gpu.GemmBackendConfig.dot_dimension_numbers)
}

// int64 batch_size = 8;
inline void GemmBackendConfig::clear_batch_size() {
  batch_size_ = int64_t{0};
}
inline int64_t GemmBackendConfig::_internal_batch_size() const {
  return batch_size_;
}
inline int64_t GemmBackendConfig::batch_size() const {
  // @@protoc_insertion_point(field_get:xla.gpu.GemmBackendConfig.batch_size)
  return _internal_batch_size();
}
inline void GemmBackendConfig::_internal_set_batch_size(int64_t value) {
  
  batch_size_ = value;
}
inline void GemmBackendConfig::set_batch_size(int64_t value) {
  _internal_set_batch_size(value);
  // @@protoc_insertion_point(field_set:xla.gpu.GemmBackendConfig.batch_size)
}

// int64 lhs_stride = 10;
inline void GemmBackendConfig::clear_lhs_stride() {
  lhs_stride_ = int64_t{0};
}
inline int64_t GemmBackendConfig::_internal_lhs_stride() const {
  return lhs_stride_;
}
inline int64_t GemmBackendConfig::lhs_stride() const {
  // @@protoc_insertion_point(field_get:xla.gpu.GemmBackendConfig.lhs_stride)
  return _internal_lhs_stride();
}
inline void GemmBackendConfig::_internal_set_lhs_stride(int64_t value) {
  
  lhs_stride_ = value;
}
inline void GemmBackendConfig::set_lhs_stride(int64_t value) {
  _internal_set_lhs_stride(value);
  // @@protoc_insertion_point(field_set:xla.gpu.GemmBackendConfig.lhs_stride)
}

// int64 rhs_stride = 11;
inline void GemmBackendConfig::clear_rhs_stride() {
  rhs_stride_ = int64_t{0};
}
inline int64_t GemmBackendConfig::_internal_rhs_stride() const {
  return rhs_stride_;
}
inline int64_t GemmBackendConfig::rhs_stride() const {
  // @@protoc_insertion_point(field_get:xla.gpu.GemmBackendConfig.rhs_stride)
  return _internal_rhs_stride();
}
inline void GemmBackendConfig::_internal_set_rhs_stride(int64_t value) {
  
  rhs_stride_ = value;
}
inline void GemmBackendConfig::set_rhs_stride(int64_t value) {
  _internal_set_rhs_stride(value);
  // @@protoc_insertion_point(field_set:xla.gpu.GemmBackendConfig.rhs_stride)
}

inline bool GemmBackendConfig::has_algorithm() const {
  return algorithm_case() != ALGORITHM_NOT_SET;
}
inline void GemmBackendConfig::clear_has_algorithm() {
  _oneof_case_[0] = ALGORITHM_NOT_SET;
}
inline GemmBackendConfig::AlgorithmCase GemmBackendConfig::algorithm_case() const {
  return GemmBackendConfig::AlgorithmCase(_oneof_case_[0]);
}
// -------------------------------------------------------------------

// BitcastBackendConfig

// .xla.LayoutProto source_layout = 1;
inline bool BitcastBackendConfig::_internal_has_source_layout() const {
  return this != internal_default_instance() && source_layout_ != nullptr;
}
inline bool BitcastBackendConfig::has_source_layout() const {
  return _internal_has_source_layout();
}
inline const ::xla::LayoutProto& BitcastBackendConfig::_internal_source_layout() const {
  const ::xla::LayoutProto* p = source_layout_;
  return p != nullptr ? *p : reinterpret_cast<const ::xla::LayoutProto&>(
      ::xla::_LayoutProto_default_instance_);
}
inline const ::xla::LayoutProto& BitcastBackendConfig::source_layout() const {
  // @@protoc_insertion_point(field_get:xla.gpu.BitcastBackendConfig.source_layout)
  return _internal_source_layout();
}
inline void BitcastBackendConfig::unsafe_arena_set_allocated_source_layout(
    ::xla::LayoutProto* source_layout) {
  if (GetArenaForAllocation() == nullptr) {
    delete reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(source_layout_);
  }
  source_layout_ = source_layout;
  if (source_layout) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:xla.gpu.BitcastBackendConfig.source_layout)
}
inline ::xla::LayoutProto* BitcastBackendConfig::release_source_layout() {
  
  ::xla::LayoutProto* temp = source_layout_;
  source_layout_ = nullptr;
#ifdef PROTOBUF_FORCE_COPY_IN_RELEASE
  auto* old =  reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(temp);
  temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  if (GetArenaForAllocation() == nullptr) { delete old; }
#else  // PROTOBUF_FORCE_COPY_IN_RELEASE
  if (GetArenaForAllocation() != nullptr) {
    temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  }
#endif  // !PROTOBUF_FORCE_COPY_IN_RELEASE
  return temp;
}
inline ::xla::LayoutProto* BitcastBackendConfig::unsafe_arena_release_source_layout() {
  // @@protoc_insertion_point(field_release:xla.gpu.BitcastBackendConfig.source_layout)
  
  ::xla::LayoutProto* temp = source_layout_;
  source_layout_ = nullptr;
  return temp;
}
inline ::xla::LayoutProto* BitcastBackendConfig::_internal_mutable_source_layout() {
  
  if (source_layout_ == nullptr) {
    auto* p = CreateMaybeMessage<::xla::LayoutProto>(GetArenaForAllocation());
    source_layout_ = p;
  }
  return source_layout_;
}
inline ::xla::LayoutProto* BitcastBackendConfig::mutable_source_layout() {
  ::xla::LayoutProto* _msg = _internal_mutable_source_layout();
  // @@protoc_insertion_point(field_mutable:xla.gpu.BitcastBackendConfig.source_layout)
  return _msg;
}
inline void BitcastBackendConfig::set_allocated_source_layout(::xla::LayoutProto* source_layout) {
  ::PROTOBUF_NAMESPACE_ID::Arena* message_arena = GetArenaForAllocation();
  if (message_arena == nullptr) {
    delete reinterpret_cast< ::PROTOBUF_NAMESPACE_ID::MessageLite*>(source_layout_);
  }
  if (source_layout) {
    ::PROTOBUF_NAMESPACE_ID::Arena* submessage_arena =
        ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper<
            ::PROTOBUF_NAMESPACE_ID::MessageLite>::GetOwningArena(
                reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(source_layout));
    if (message_arena != submessage_arena) {
      source_layout = ::PROTOBUF_NAMESPACE_ID::internal::GetOwnedMessage(
          message_arena, source_layout, submessage_arena);
    }
    
  } else {
    
  }
  source_layout_ = source_layout;
  // @@protoc_insertion_point(field_set_allocated:xla.gpu.BitcastBackendConfig.source_layout)
}

// .xla.LayoutProto result_layout = 2;
inline bool BitcastBackendConfig::_internal_has_result_layout() const {
  return this != internal_default_instance() && result_layout_ != nullptr;
}
inline bool BitcastBackendConfig::has_result_layout() const {
  return _internal_has_result_layout();
}
inline const ::xla::LayoutProto& BitcastBackendConfig::_internal_result_layout() const {
  const ::xla::LayoutProto* p = result_layout_;
  return p != nullptr ? *p : reinterpret_cast<const ::xla::LayoutProto&>(
      ::xla::_LayoutProto_default_instance_);
}
inline const ::xla::LayoutProto& BitcastBackendConfig::result_layout() const {
  // @@protoc_insertion_point(field_get:xla.gpu.BitcastBackendConfig.result_layout)
  return _internal_result_layout();
}
inline void BitcastBackendConfig::unsafe_arena_set_allocated_result_layout(
    ::xla::LayoutProto* result_layout) {
  if (GetArenaForAllocation() == nullptr) {
    delete reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(result_layout_);
  }
  result_layout_ = result_layout;
  if (result_layout) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:xla.gpu.BitcastBackendConfig.result_layout)
}
inline ::xla::LayoutProto* BitcastBackendConfig::release_result_layout() {
  
  ::xla::LayoutProto* temp = result_layout_;
  result_layout_ = nullptr;
#ifdef PROTOBUF_FORCE_COPY_IN_RELEASE
  auto* old =  reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(temp);
  temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  if (GetArenaForAllocation() == nullptr) { delete old; }
#else  // PROTOBUF_FORCE_COPY_IN_RELEASE
  if (GetArenaForAllocation() != nullptr) {
    temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  }
#endif  // !PROTOBUF_FORCE_COPY_IN_RELEASE
  return temp;
}
inline ::xla::LayoutProto* BitcastBackendConfig::unsafe_arena_release_result_layout() {
  // @@protoc_insertion_point(field_release:xla.gpu.BitcastBackendConfig.result_layout)
  
  ::xla::LayoutProto* temp = result_layout_;
  result_layout_ = nullptr;
  return temp;
}
inline ::xla::LayoutProto* BitcastBackendConfig::_internal_mutable_result_layout() {
  
  if (result_layout_ == nullptr) {
    auto* p = CreateMaybeMessage<::xla::LayoutProto>(GetArenaForAllocation());
    result_layout_ = p;
  }
  return result_layout_;
}
inline ::xla::LayoutProto* BitcastBackendConfig::mutable_result_layout() {
  ::xla::LayoutProto* _msg = _internal_mutable_result_layout();
  // @@protoc_insertion_point(field_mutable:xla.gpu.BitcastBackendConfig.result_layout)
  return _msg;
}
inline void BitcastBackendConfig::set_allocated_result_layout(::xla::LayoutProto* result_layout) {
  ::PROTOBUF_NAMESPACE_ID::Arena* message_arena = GetArenaForAllocation();
  if (message_arena == nullptr) {
    delete reinterpret_cast< ::PROTOBUF_NAMESPACE_ID::MessageLite*>(result_layout_);
  }
  if (result_layout) {
    ::PROTOBUF_NAMESPACE_ID::Arena* submessage_arena =
        ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper<
            ::PROTOBUF_NAMESPACE_ID::MessageLite>::GetOwningArena(
                reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(result_layout));
    if (message_arena != submessage_arena) {
      result_layout = ::PROTOBUF_NAMESPACE_ID::internal::GetOwnedMessage(
          message_arena, result_layout, submessage_arena);
    }
    
  } else {
    
  }
  result_layout_ = result_layout;
  // @@protoc_insertion_point(field_set_allocated:xla.gpu.BitcastBackendConfig.result_layout)
}

#ifdef __GNUC__
  #pragma GCC diagnostic pop
#endif  // __GNUC__
// -------------------------------------------------------------------

// -------------------------------------------------------------------


// @@protoc_insertion_point(namespace_scope)

}  // namespace gpu
}  // namespace xla

// @@protoc_insertion_point(global_scope)

#include <google/protobuf/port_undef.inc>
#endif  // GOOGLE_PROTOBUF_INCLUDED_GOOGLE_PROTOBUF_INCLUDED_tensorflow_2fcompiler_2fxla_2fservice_2fgpu_2fbackend_5fconfigs_2eproto
