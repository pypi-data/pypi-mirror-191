{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# External Libs\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import os\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../../..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zeno_etl_libs.helper.aws.s3 import S3\n",
    "from zeno_etl_libs.logger import get_logger\n",
    "from zeno_etl_libs.db.db import DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set env\n",
    "env = \"stage\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['env'] = env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = get_logger(level=\"INFO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(f\"env: {env}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import modified dataset and category splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = S3(bucket_name=\"sagemaker-ap-south-1-921939243643\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_full_path = s3.download_file_from_s3(file_name=\"data/modified_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(csv_full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df1.loc[3:27, :]\n",
    "df2 = df2.drop(['Unnamed: 0'], axis=1)\n",
    "df2['Dates'] = pd.to_datetime(df2['Dates']) # converting to datetime format\n",
    "df2 = df2.set_index('Dates')\n",
    "df2.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train - Test Split Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>for_valid=True</b> implies, the split is for validation. In this case, all data before March 2021, is taken the last three months in the obtained dataset is taken as Test set, and all the prior datapoints are taken into Train set.\n",
    "\n",
    "<b>for_valid=False</b> implies the split is for final model. Hence all the datapoints before March 2021 is taken into Train set. The data of March 2021 is only considered into Test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_train_split(drug_id, for_valid=True):\n",
    "    df3 = df2[[drug_id]]\n",
    "    if for_valid:\n",
    "        train = df3[0:-5] # Training Split\n",
    "        test = df3[-5:-2] # Testing Split\n",
    "    else:\n",
    "        train = df3[0:-2] # Training Split\n",
    "        test = df3[-2:-1] # For 2021 March\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Libraries -- Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from statsmodels.tsa.api import SimpleExpSmoothing, Holt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Exponential Smoothening Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple exponential smoothing model. The <i><b>smoothing constant</b></i> hyperparameter is not provided, so that the algorithm finds the best parameter itself. This model does not capture any trend, it simply forecasts a constant value for all the future predictions. The forecasting is performed by giving higher weightage to the most recent values decided by the <i><b>smoothing constant</b></i>. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exp_mod(drug_id, for_valid=True):\n",
    "    train, test = test_train_split(drug_id, for_valid)\n",
    "    fit = SimpleExpSmoothing(train).fit() # Optimum alpha automatically computed\n",
    "    if for_valid:\n",
    "        fcast = fit.forecast(len(test))\n",
    "        mae = mean_absolute_error(test, fcast)\n",
    "        return mae\n",
    "    else:\n",
    "        fcast = fit.forecast(1)\n",
    "        return int(math.ceil(fcast.values[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result_1 = exp_mod('216583') # for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result_2 = exp_mod('216583', for_valid=False) # for testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Holts Winter Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is better model, due to its ability to capture trend. In order to fit the model, a <b><i>linear trend</i></b> is assumed. The hyperparameter values such as  <i><b>smoothing_level</b></i> and <i><b>smoothing_trend</b></i> are also assumed to be <b>0.8</b> and <b>0.2</b> respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def holts_mod(drug_id, for_valid=True):\n",
    "    train, test = test_train_split(drug_id, for_valid)\n",
    "    fit = Holt(train).fit(smoothing_level=0.8, smoothing_trend=0.2, optimized=False) # assume hyp-params and linear trend\n",
    "    if for_valid:\n",
    "        fcast = fit.forecast(len(test))\n",
    "        mae = mean_absolute_error(test, fcast)\n",
    "        return mae\n",
    "    else:\n",
    "        fcast = fit.forecast(1)\n",
    "        return int(math.ceil(fcast.values[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result_3 = holts_mod('216583') # for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result_4 = holts_mod('216583', for_valid=False) # for testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define LSTM Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A basic LSTM model, much more powerfull than previous models. Uses special class of Neural Networks to train the model. The hyperparametrs such as <i><b>number of layers, number of neurons, activation function, optimizers</b></i> and <i><b>number of epochs</b></i> are assumed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing independent and dependent features\n",
    "def prepare_lstm_data(timeseries_data, n_features):\n",
    "    X, y =[],[]\n",
    "    for i in range(len(timeseries_data)):\n",
    "        # find the end of this pattern\n",
    "        end_ix = i + n_features\n",
    "        # check if we are beyond the sequence\n",
    "        if end_ix > len(timeseries_data)-1:\n",
    "            break\n",
    "        # gather input and output parts of the pattern\n",
    "        seq_x, seq_y = timeseries_data[i:end_ix], timeseries_data[end_ix]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_mod(drug_id, for_valid=True, n_steps=3, epochs=500):\n",
    "    n_features = 1\n",
    "    train, test = test_train_split(drug_id, for_valid)\n",
    "    X, y = prepare_lstm_data(train.values, n_steps) # function call\n",
    "    X = X.reshape((X.shape[0], X.shape[1], n_features))\n",
    "    \n",
    "    # define model\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(50, activation='relu', return_sequences=True, input_shape=(n_steps, n_features)))\n",
    "    model.add(LSTM(50, activation='relu'))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    # fit model\n",
    "    model.fit(X, y, epochs=epochs, verbose=0)\n",
    "    \n",
    "    if for_valid:\n",
    "        # demonstrate prediction for next n days\n",
    "        x_input = train[-(len(test)):][drug_id].values\n",
    "        temp_input=list(x_input)\n",
    "        lst_output=[]\n",
    "        i=0\n",
    "        while(i<len(test)):\n",
    "            if(len(temp_input)>n_steps):\n",
    "                x_input=np.array(temp_input[1:])\n",
    "                x_input = x_input.reshape((1, n_steps, n_features))\n",
    "                yhat = model.predict(x_input, verbose=0)\n",
    "                temp_input.append(yhat[0][0])\n",
    "                temp_input=temp_input[1:]\n",
    "                lst_output.append(yhat[0][0])\n",
    "                i=i+1\n",
    "            else:\n",
    "                x_input = x_input.reshape((1, n_steps, n_features))\n",
    "                yhat = model.predict(x_input, verbose=0)\n",
    "                temp_input.append(yhat[0][0])\n",
    "                lst_output.append(yhat[0][0])\n",
    "                i=i+1\n",
    "    \n",
    "        #converting to dictionary --> df\n",
    "        pred_dict = {'Dates': ['2020-12-01', '2021-01-01', '2021-02-01'], # assuming forecasting for 3 days\n",
    "                    'Sales': lst_output}\n",
    "        pred_df = pd.DataFrame(pred_dict)\n",
    "        pred_df['Dates'] = pd.to_datetime(pred_df['Dates']) # converting to datetime format\n",
    "        pred_df = pred_df.set_index('Dates')\n",
    "        mae = mean_absolute_error(test, pred_df)\n",
    "        return mae\n",
    "    \n",
    "    else:\n",
    "        x_input = train[-3:][drug_id].values\n",
    "        x_input = x_input.reshape((1, 3, 1))\n",
    "        yhat = model.predict(x_input, verbose=0)\n",
    "        return int(math.ceil(yhat[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result_5 = lstm_mod('216583') # for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result_6 = lstm_mod(drug_id='216583', for_valid=False) # for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "tf.config.list_physical_devices('GPU')\n",
    "(ds_train, ds_test), ds_info = tfds.load(\n",
    "    'mnist',\n",
    "    split=['train', 'test'],\n",
    "    shuffle_files=True,\n",
    "    as_supervised=True,\n",
    "    with_info=True,\n",
    ")\n",
    "def normalize_img(image, label):\n",
    "  \"\"\"Normalizes images: `uint8` -> `float32`.\"\"\"\n",
    "  return tf.cast(image, tf.float32) / 255., label\n",
    "batch_size = 128\n",
    "ds_train = ds_train.map(\n",
    "    normalize_img, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "ds_train = ds_train.cache()\n",
    "ds_train = ds_train.shuffle(ds_info.splits['train'].num_examples)\n",
    "ds_train = ds_train.batch(batch_size)\n",
    "ds_train = ds_train.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "ds_test = ds_test.map(\n",
    "    normalize_img, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "ds_test = ds_test.batch(batch_size)\n",
    "ds_test = ds_test.cache()\n",
    "ds_test = ds_test.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu'),\n",
    "  tf.keras.layers.Conv2D(64, kernel_size=(3, 3),\n",
    "                 activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "#   tf.keras.layers.Dropout(0.25),\n",
    "  tf.keras.layers.Flatten(),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "#   tf.keras.layers.Dropout(0.5),\n",
    "  tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "model.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "model.fit(\n",
    "    ds_train,\n",
    "    epochs=6,\n",
    "    validation_data=ds_test,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write result to DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = DB(read_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.open_connection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_result = f\"result_5: {datetime.datetime.now()}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = f\"\"\" \n",
    "        insert into \"prod2-generico\".\"temp-str\" (col1) values ('Hello at {datetime.datetime.now()}: {all_result}')\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.execute(query=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.close_connection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
