Metadata-Version: 2.1
Name: huggingfaceinference
Version: 0.6
Summary: Simple inference usecases using hugging transformers library on varied use cases of the AI/ML Domain
Home-page: https://github.com/Vishnunkumar/huggingfaceinference/
Author: Vishnu Nandakumar
Author-email: nkumarvishnu25@gmail.com
License: MIT license
Keywords: huggingfaceinference
Classifier: Development Status :: 3 - Alpha
Classifier: Intended Audience :: Developers
Classifier: Topic :: Software Development :: Build Tools
Classifier: License :: OSI Approved :: MIT License
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.4
Classifier: Programming Language :: Python :: 3.5
Classifier: Programming Language :: Python :: 3.6
Description-Content-Type: text/markdown
License-File: LICENSE

# huggingfaceinference
Simple inference pipelines using hugging transformers library and finetuned tiny models. Will be highly useful on scenarios where we need to optimize storage and memory cost with a compensation in accuracy.

# Installation
```pip install huggingfaceinference```

# Implementation

## gramcorrector

A simple grammatical and spelling resolver using huggingface transformers. The Dataset (around 50k) is retrieved from kaggle and corrupted using random substution of letters in words for finetuning. The base model used was _google/t5-efficient-tiny-nl32_ as it is highly compact and efficient(<250 MB).(was modeled on fp32 to reduce drop in efficiency). 
```python

from huggingfaceinference.pipeline import TinyGram

tg = TinyGram()
tg.gramcorrector("What do you think I shold be doing", n=3)

""" Output: ['What do you think I need to be doing',
 'what do you think I should be doing',
 'what do you think I will be doing']"""

# The above pipeline is built by finetuing "google/t5-efficient-tiny-nl32" model on sentences which are corrupted by random noising.
```
