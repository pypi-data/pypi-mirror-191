# Copyright (c) 2023 Max Bane <max@thebanes.org>
#
# Redistribution and use in source and binary forms, with or without modification, are
# permitted provided that the following conditions are met:
#
# 1. Redistributions of source code must retain the above copyright notice, this list of
# conditions and the following disclaimer.
#
# 2. Redistributions in binary form must reproduce the above copyright notice, this list
# of conditions and the following disclaimer in the documentation and/or other materials
# provided with the distribution.
#
# Subject to the terms and conditions of this license, each copyright holder and
# contributor hereby grants to those receiving rights under this license a perpetual,
# worldwide, non-exclusive, no-charge, royalty-free, irrevocable (except for failure to
# satisfy the conditions of this license) patent license to make, have made, use, offer
# to sell, sell, import, and otherwise transfer this software, where such license
# applies only to those patent claims, already acquired or hereafter acquired,
# licensable by such copyright holder or contributor that are necessarily infringed by:
#
# (a) their Contribution(s) (the licensed copyrights of copyright holders and
# non-copyrightable additions of contributors, in source or binary form) alone; or
#
# (b) combination of their Contribution(s) with the work of authorship to which such
# Contribution(s) was added by such copyright holder or contributor, if, at the time the
# Contribution is added, such addition causes such combination to be necessarily
# infringed. The patent license shall not apply to any other combinations which include
# the Contribution.
#
# Except as expressly stated above, no rights or licenses from any copyright holder or
# contributor is granted under this license, whether expressly, by implication, estoppel
# or otherwise.
#
# DISCLAIMER
#
# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" AND ANY
# EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
# OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT
# SHALL THE COPYRIGHT HOLDERS OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT,
# INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
# TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR
# BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY
# WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH
# DAMAGE.

"""
The core building blocks and utilities of the Frankenfit library, including
Transform, FitTransform, HP, and friends.

Ordinarily, users should never need to import this module directly. Instead, they access
the classes and functions defined here through the public API exposed as
``frankenfit.*``.
"""

from __future__ import annotations

import copy
import inspect
import logging
import warnings
from abc import ABC, abstractmethod
from contextlib import contextmanager
from functools import wraps
from typing import (
    Any,
    Callable,
    ClassVar,
    Generic,
    Iterable,
    Iterator,
    Literal,
    Optional,
    Sequence,
    Sized,
    Type,
    TypeVar,
    Union,
    cast,
    overload,
)

import graphviz  # type: ignore
from attrs import NOTHING, Factory, define, field, fields_dict

from .params import HP, Bindings, UnresolvedHyperparameterError, params

_LOG = logging.getLogger(__name__)

T = TypeVar("T")
T_co = TypeVar("T_co", covariant=True)
P = TypeVar("P", bound="Pipeline")
R = TypeVar("R", bound="Transform")
B = TypeVar("B", bound="Backend")
R_co = TypeVar("R_co", covariant=True, bound="Transform")
State = TypeVar("State")
State_co = TypeVar("State_co", covariant=True)
DataType = TypeVar("DataType")


def is_iterable(obj):
    """
    Utility function to test if an object is iterable.
    """
    try:
        iter(obj)
    except TypeError:
        return False
    return True


# last auto-generated tag number for a given Transform class name. Used for
# autogenerated tags.
_id_num: dict[str, int] = {}


def _next_id_num(class_name: str) -> int:
    n = _id_num.get(class_name, 0)
    n += 1
    _id_num[class_name] = n
    return n


def _next_auto_tag(partial_self: Transform) -> str:
    """
    Autogenerate a tag for the given Transform object, which is presumably only
    partially constructed. Ensures that Transforms receive unique default
    tags when not specified by the user.

    The current implementation returns the object's class qualname with ``"#N"``
    appended, where N is incremented (per class name) on each call.
    """
    class_name = partial_self.__class__.__qualname__
    return str(_next_id_num(class_name))


DEFAULT_VISUALIZE_DIGRAPH_KWARGS = {
    "node_attr": {
        "shape": "box",
        "fontsize": "10",
        "fontname": "Monospace",
    },
    "edge_attr": {"fontsize": "10", "fontname": "Monospace"},
}
"""
The default keyword arguments passed to the ``graphviz.Digraph()`` constructor by
:meth:`Transform.visualize() <frankenfit.Transform.visualize>`.
"""

#######################################################################
# Computational backend logic


class Future(Generic[T_co], ABC):
    @abstractmethod
    def result(self) -> T_co:
        raise NotImplementedError  # pragma: no cover

    @abstractmethod
    def belongs_to(self, backend: Backend) -> bool:
        raise NotImplementedError  # pragma: no cover

    @abstractmethod
    def __eq__(self, o):
        raise NotImplementedError  # pragma: no cover


@define
class Backend(ABC):
    trace: tuple[str, ...] = tuple()

    @abstractmethod
    def put(self, data: T) -> Future[T]:
        raise NotImplementedError  # pragma: no cover

    @abstractmethod
    def submit(
        self,
        key_prefix: str,
        function: Callable,
        *function_args,
        pure: bool = True,
        **function_kwargs,
    ) -> Future[Any]:
        raise NotImplementedError  # pragma: no cover

    def maybe_put(self, data: T | Future[T] | None) -> Future[T] | None:
        if data is None:
            return data
        if isinstance(data, Future) and data.belongs_to(self):
            return data
        # we need to put it...
        if isinstance(data, Future):
            # but it's a future from some other backend, so first we materialize it
            data = cast(Future[T], data).result()
            return self.maybe_put(data)
        return self.put(data)

    def push_trace(self: B, name: str) -> B:
        self_copy = copy.copy(self)
        self_copy.trace += (name,)
        return self_copy

    @contextmanager
    def submitting_from_transform(self: B, name: str = "") -> Iterator[B]:
        yield self.push_trace(name) if name else self

    def fit(
        self,
        transform: Transform[DataType],
        data_fit: Optional[DataType | Future[DataType]] = None,
        bindings: Optional[Bindings] = None,
        /,
        **kwargs,
    ) -> FitTransform[Transform[DataType], DataType]:
        bindings = {**(bindings or {}), **kwargs}
        if isinstance(data_fit, Sized):
            data_len = len(data_fit)
        else:
            data_len = None
        _LOG.debug(
            f"{self}: Fitting {transform} on {type(data_fit)}"
            f"{f' (len={data_len})' if data_len is not None else ''} "
            f"with bindings={bindings!r}."
        )

        data_fit = cast(Union[DataType, Future[DataType], None], data_fit)
        transform = transform.resolve(bindings).on_backend(self)
        state = transform._submit_fit(data_fit, bindings)
        return type(transform).fit_transform_class(transform, state, bindings)

    @overload
    def apply(
        self,
        what: FitTransform[R, DataType],
        data_apply: Optional[DataType | Future[DataType]] = None,
        /,
    ) -> Future[DataType]:
        ...  # pragma: no cover

    @overload
    def apply(
        self,
        what: StatelessTransform[DataType],
        data_apply: Optional[DataType | Future[DataType]] = None,
        bindings: Optional[Bindings] = None,
        /,
        **kwargs,
    ) -> Future[DataType]:
        ...  # pragma: no cover

    @overload
    def apply(
        self,
        what: Pipeline[DataType],
        data_apply: Optional[DataType | Future[DataType]] = None,
        bindings: Optional[Bindings] = None,
        /,
        **kwargs,
    ) -> Future[DataType]:
        ...  # pragma: no cover

    def apply(
        self,
        what: (
            FitTransform[R, DataType]
            | StatelessTransform[DataType]
            | Pipeline[DataType]
        ),
        data_apply: Optional[DataType | Future[DataType]] = None,
        bindings: Optional[Bindings] = None,
        /,
        **kwargs,
    ) -> Future[DataType]:
        bindings = {**(bindings or {}), **kwargs}
        data: Any = data_apply
        if isinstance(data, Sized):
            data_len = len(data)
        else:
            data_len = None

        _LOG.debug(
            f"{self}: Applying {what} on {type(data_apply)}"
            f"{f' (len={data_len})' if data_len is not None else ''}"
            f"{f', with bindings={bindings!r}' if bindings is not None else ''}."
        )

        if isinstance(what, StatelessTransform):
            data = self.maybe_put(data)  # so that it doesn't get put twice
            fit_what = self.fit(what, data, bindings)
            return self.apply(fit_what, data)

        if isinstance(what, Pipeline):
            result = (
                what.resolve(bindings)
                .on_backend(self)
                ._submit_fit(data, bindings, return_what="result")
            )
            assert isinstance(result, Future)  # type narrowing
            return result

        if not isinstance(what, FitTransform):  # pragma: no cover
            raise TypeError(
                f"{self}: cannot apply object: {what!r}. Please provide a "
                "FitTransform, Pipeline, or StatelessTransform."
            )
        assert isinstance(what, FitTransform)
        if len(bindings):
            raise TypeError(
                "Backend.apply: bindings must not be supplied when applying "
                "a FitTransform (its hyperparameters have already been resolved by "
                "fit-time bindings)."
            )
        data_result = what.on_backend(self)._submit_apply(data)
        if data_result is None:  # pragma: no cover
            return self.put(None)  # type: ignore [arg-type]
        return data_result


@define
class LocalFuture(Generic[T_co], Future[T_co]):
    obj: T_co

    def result(self) -> T_co:
        return self.obj

    def belongs_to(self, backend: Backend) -> bool:
        return isinstance(backend, LocalBackend)

    def __repr__(self):
        return f"LocalFuture(obj={type(self.obj)})"

    def __eq__(self, other):
        if type(self) is not type(other):
            return False
        other = cast(LocalFuture, other)
        my_obj = self.obj
        other_obj = other.obj
        try:
            return my_obj.equals(other_obj)
        except AttributeError:
            pass
        try:
            return (my_obj == other_obj).all()
        except AttributeError:
            pass
        return my_obj == other_obj


@define
class LocalBackend(Backend):
    def put(self, data: T) -> LocalFuture[T]:
        return LocalFuture(data)

    def submit(
        self,
        key_prefix: str,
        function: Callable,
        *function_args,
        pure: bool = True,
        **function_kwargs,
    ) -> LocalFuture[Any]:
        # materialize any Future args (possibly from other backends)
        function_args = tuple(
            (a.result() if isinstance(a, Future) else a) for a in function_args
        )
        function_kwargs = {
            k: (v.result() if isinstance(v, Future) else v)
            for k, v in function_kwargs.items()
        }
        result = function(*function_args, **function_kwargs)
        return LocalFuture(result)


LOCAL_BACKEND = LocalBackend()


#######################################################################
# Core base classes


class PipelineMember:
    """
    Abstract base class of :class:`Transform` and :class:`FitTransform`, specifying the
    functionality common to both of them, namely:

    - concatenation (:meth:`then`, :meth:`__add__`),
    - selecting children by name (:data:`name`, :meth:`find_by_name`,
      :meth:`_children`), and
    - visualization (:meth:`_visualize`).
    """

    def then(
        self,
        other: PipelineMember | Sequence[PipelineMember] | None = None,
    ) -> Pipeline:
        """
        Implements concatenation for :class:`Transforms <Transform>` and
        :class:`FitTransforms <FitTransform>`.

        Return a :class:`Pipeline` containing the concatenation of ``self`` and
        ``other``.  The addition operator is an alias for this method, meaning that the
        following are equivalent pairs::

            t1: Transform | FitTransform
            t2: Transform | FitTransform

            t1 + t2 == t1.then(t2)
            t1 + [t2, ...] == t1.then([t2, ...])

        In the case of appending built-in ``Transform`` classes to a ``Pipeline`` it is
        usually not necessary to call ``then()`` because the ``Pipeline`` subclasses
        :class:`~frankenfit.UniversalPipeline`, :class:`~frankenfit.DataFramePipeline`
        have more specific call-chain methods for each built-in ``Transform``. For
        example, instead of::

            p: DataFramePipeline
            p.then([ff.dataframe.Winsorize(...), ff.dataframe.DeMean(...)])

        ...it is more idiomatic to write::

            p.winsorize(...).de_mean(...)

        The main use cases for ``then()`` are to append user-defined ``Transform``
        subclasses that don't have call-chain methods like the above (but see
        :class:`Pipeline.with_methods <frankenfit.Pipeline.with_methods>` for a pleasing
        alternative), and to append separately constructed ``Pipeline`` objects when
        writing a pipeline in the call-chain style. For example::

            def bake_features(cols):
                # using built-in methods for Winsorize and ZScore transforms
                return ff.DataFramePipeline().winsorize(0.05, cols).z_score(cols)

            class MyCustomTransform(ff.Transform):
                ...

            pipeline = (
                ff.DataFramePipeline()
                .pipe(np.log1p, ['carat'])  # built-in method for Pipe transform
                .then(bake_features(['carat', 'table', 'height']))  # append Pipeline
                .then(MyCustomTransform(...))  # append a user-defined transform
            )

        Another common use case for ``then()`` is when you want a grouping transform
        like :meth:`~frankenfit.DataFramePipelineInterface.group_by_cols()` to group a
        complex sub-pipeline, not just a single transform, e.g.::

            pipeline = (
                ff.DataFramePipeline()
                .group_by_cols("cut")
                    .then(
                        # This whole pipeline of transforms will be fit and applied
                        # independently per distinct value of cut
                        ff.DataFramePipeline()
                        .zscore(["carat", "table", "depth"])
                        .winsorize(0.05, ["carat", "table", "depth"])
                    )
            )

        Note
        ----
        Subclasses :class:`~frankenfit.universal.UniversalTransform`,
        :class:`~frankenfit.universal.FitUniversalTransform`,
        :class:`~frankenfit.dataframe.DataFrameTransform`, and
        :class:`~frankenfit.dataframe.FitDataFrameTransform` override ``then()`` to
        return :class:`~frankenfit.UniversalPipeline` or
        :class:`~frankenfit.DataFramePipeline` as appropriate.

        This means ``then()`` can be used to initiate a call-chain sequence if one is
        starting with a bare ``UniversalTransform`` or ``DataFrameTransform`` instance
        outside of a pipeline. For example::

            # suppose we have a bare DeMean object from somewhere...
            de_mean = ff.dataframe.DeMean("foo")
            # we can start a DataFramePipeline by calling then()
            my_pipeline = (
                de_mean
                .then()
                .winsorize(0.05)
                .pipe(np.sqrt, "bar")
            )

        Parameters
        ----------
        other
            The :class:`Transform` or :class:`FitTransform` instance which will be
            concatenated to ``self``, or a list of the same, which will be concatenated
            in the order in which in they appear in the list, or ``None``, in which case
            a pipeline containing only ``self`` is returned.

        Raises
        ------
        ``TypeError``
            If ``other`` has the wrong type.


        :rtype: :class:`frankenfit.core.Pipeline`
        """
        transforms: Sequence[PipelineMember]
        if other is None:
            transforms = [self]
        elif isinstance(other, list):
            transforms = [self] + other
        elif isinstance(other, PipelineMember):
            transforms = [self, other]
        else:
            raise TypeError(
                f"then(): other must be PipelineMember or list, got: {other!r}"
            )

        return Pipeline(transforms=transforms)

    def __add__(
        self, other: PipelineMember | Sequence[PipelineMember] | None
    ) -> Pipeline:
        """
        Syntactic sugar for :meth:`then()`.
        """
        return self.then(other)

    @property
    @abstractmethod
    def name(self) -> str:
        """
        Subclasses of :class:`PipelineMember` must implement a ``name`` property.

        .. SEEALSO:: :meth:`find_by_name()`
        """
        raise NotImplementedError

    @abstractmethod
    def _children(self) -> Iterable[PipelineMember]:
        """
        Return an iterator of child ``PipelineMember`` instances. Subclasses must
        implement. Must yield at least ``self``.
        """
        raise NotImplementedError

    def find_by_name(self, name: str) -> PipelineMember:
        """
        Recurse through child objects (as yielded by :meth:`_children`) and return the
        first one with the given ``name``. If not found, raise ``KeyError``.

        Parameters
        ----------
        name:
            The ``name`` of the desired child object, e.g., ``"DeMean#5"``.

        Raises
        ------
        KeyError
            No child object was found with the given ``name``.
        """
        for child in self._children():
            if child.name == name:
                return child
        raise KeyError(f"No child found with name: {name}")

    @abstractmethod
    def _visualize(self, digraph, bg_fg: tuple[str, str]) -> tuple[list, list]:
        """
        Subclasses must implement this for visualiztion black magic.
        """
        raise NotImplementedError


class FitTransform(Generic[R_co, DataType], PipelineMember):
    """
    The result of fitting a :class:`Transform`. Call this object's
    :meth:`apply()` method on some data to get the result of applying the
    now-fit transformation.

    The ``Transform`` object that was fit, with any hyperparameters resolved
    against whatever bindings were provided at fit-time, is available as
    :meth:`resolved_transform()`.

    The fit state of the transformation, as returned by the ``Transform``
    object's ``_fit()`` method at fit-time, is available from :meth:`state()`,
    and this is the state that will be used at apply-time (i.e., passed as the
    ``state`` argument of the Transform's ``_apply()`` method).

    Frankenfit users should never need to construct a ``FitTransform`` object
    themselves. They are always obtained from :meth:`Transform.fit()` or
    :meth:`Backend.fit()`.
    """

    _Self = TypeVar("_Self", bound="FitTransform")

    def __init__(
        self,
        resolved_transform: R_co,
        state: Any,
        bindings: Optional[Bindings] = None,
    ):
        self.__resolved_transform = resolved_transform
        self.__state = state
        self.__bindings = bindings or {}
        self.backend: Backend = resolved_transform.backend

    @property
    def name(self) -> str:
        """
        The ``name`` of a ``FitTransform`` is the same as that of the ``Transform`` that
        was fit to obtain it.
        """
        return self.__resolved_transform.name

    def __str__(self) -> str:
        return f"FitTransform[{self.name}]"

    def __repr__(self):
        return (
            f"{self.__class__.__name__}("
            f"resolved_transform={self.__resolved_transform!r}, "
            f"state={type(self.__state)!r}, "
            f"bindings={self.__bindings!r}"
            f")"
        )

    def __eq__(self, other: object) -> bool:
        if type(self) is not type(other):
            return False
        other = cast(FitTransform, other)
        if self.resolved_transform() != other.resolved_transform():
            return False
        my_state = self.state()
        other_state = other.state()
        try:
            return my_state.equals(other_state)
        except AttributeError:
            pass
        try:
            return (my_state == other_state).all()
        except AttributeError:
            pass
        return my_state == other_state

    def _submit_apply(
        self,
        data_apply: Optional[DataType | Future[DataType]] = None,
    ) -> Future[DataType] | None:
        tf = self.resolved_transform().on_backend(self.backend)
        return tf._submit_apply(data_apply, self.state())

    def apply(
        self,
        data_apply: Optional[DataType | Future[DataType]] = None,
    ) -> DataType:
        """
        Return the result of applying this FitTransform to the given data.
        """
        # Convenience method that applies this transform on its backend and
        # materializes the result
        return self.backend.apply(self, data_apply).result()

    # TODO: refit()? incremental_fit()?

    def resolved_transform(self) -> R_co:
        """
        Return the Transform that was fit to produce this FitTransform, with all
        hyperparameters resolved to their fit-time bindings.
        """
        return self.__resolved_transform

    def on_backend(self: _Self, backend: Backend) -> _Self:
        self_copy = copy.copy(self)
        self_copy.backend = backend
        return self_copy

    def bindings(self) -> Bindings:
        """
        Return the bindings dict according to which the transformation's hyperparameters
        were resolved.
        """
        return self.__bindings

    def state(self) -> Any:
        """
        Return the fit state of the transformation, which is an arbitrary object
        determined by the implementation of the ``Transform`` that was fit to obtain
        this ``FitTransform`` instance.
        """
        return self.__state

    def materialize_state(self: _Self) -> _Self:
        tf = self.resolved_transform()
        state = tf._materialize_state(self.state())
        return type(self)(tf, state, self.bindings())

    def _children(self) -> Iterable[PipelineMember]:
        """
        Recursively searches :meth:`self.state() <FitTransform.state>` for
        ``FitTransforms`` and iterables thereof, yielding them in the order found.
        Subclasses should override this if they have other ways of keeping child
        transforms, so that they are discoverable by
        :meth:`~PipelineMember.find_by_name()`.
        """
        mat_self = self.materialize_state()
        yield mat_self
        val = self.state()
        if isinstance(val, PipelineMember):
            yield from val._children()
        elif is_iterable(val) and not isinstance(val, str):
            for x in val:
                if isinstance(x, PipelineMember):
                    yield from x._children()

    def _visualize(self, digraph, bg_fg: tuple[str, str]) -> tuple[list, list]:
        # TODO: flesh out more
        self_label = f"FitTransform: {self.name}"
        digraph.node(self.name, label=self_label)
        return ([self.name], [(self.name, "")])


@params
class Transform(ABC, Generic[DataType], PipelineMember):
    """
    The abstract base class of all Transforms. The simplest way to implement a subclass
    is to implement the :meth:`_fit()` and :meth:`_apply()` methods (but see
    :class:`StatelessTransform`, which removes the requirement to implement
    :meth:`_fit()`).

    Subclasses use `attrs <https://www.attrs.org>`_ field variables to hold
    parameters (but not fit state) of the transformation being implemented, with the
    expectation that these parameters will be provided by the user of the subclass as
    constructor arguments.  Thanks to ``attrs``, in most cases no constructor needs to
    be written explicitly by the subclass author, and in any case only ``attrs``-managed
    field variables will be treated as potential hyperparameters at fit-time (i.e., to
    potentially get their values from the ``bindings=`` kwarg to :meth:`fit()`).

    If declaring any parameters, subclasses must apply the :func:`params()` decorator,
    which is a Frankenfit-specific wrapper around ``attrs.define``.

    The implementations of :meth:`_fit()` and :meth:`_apply()` may refer freely to any
    ``attrs`` fields (generally understood as parameters of the transformation) as
    instance variables on ``self``. If any fields were given as hyperparameters at
    construction time, they are resolved to concrete bindings before ``_fit()`` and
    ``_apply()`` are invoked.

    ``_fit()`` should accept some training data and return an arbitrary object
    representing fit state, which will be passed to ``_apply()`` at apply-time.
    Generally speaking, ``_fit()`` should *not* mutate anything about ``self`` or the
    training data.

    ``_apply()`` should then accept a state object as returned by ``_fit()`` and return
    the result of applying the transformation to some given apply-time data, also
    without mutating ``self`` or the data.

    Once implemented, the subclass is used like any ``Transform``, which is to say by
    constructing an instance with some parameters (which may be hypeparameters), and
    then calling its ``fit()`` and ``apply()`` methods (note no leading underscores).

    .. SEEALSO::
        See the section :doc:`implementing_transforms` of the :doc:`Frankenfit
        documentation <cover>` for more information.

    Parameters
    ----------
    tag : str
        All Transform subclasses accept an optional `tag` param that is used for naming
        and selecting Transforms that belong to Pipelines. If not supplied, a default
        value is creating based on the Transform's class name and a nonce value.

        .. SEEALSO::
            :meth:`find_by_name`, :meth:`FitTransform.find_by_name`.

    Examples
    --------
    An example of writing a ``Transform`` subclass::

        import pandas as pd
        import frankenfit as ff

        # A simple stateful transform from scratch
        @ff.params
        class DeMean(ff.Transform):
            "De-mean some columns."

            cols: list[str]

            def _fit(self, df_fit: pd.DataFrame) -> object:
                return df_fit[self.cols].mean()

            def _apply(self, df_apply: pd.DataFrame, state: object):
                means = state
                return df_apply.assign(**{
                    c: df_apply[c] - means[c]
                    for c in self.cols
                })

    An example of a stateless Transform whose only parameter is a list of columns::

        @ff.params
        class Select(ff.StatelessTransform):
            cols: list[str]

            def _apply(
                self, df_apply: pd.DataFrame, state: Any=None
            ) -> pd.DataFrame:
                return df_apply[self.cols]
    """

    fit_transform_class: ClassVar[Type[FitTransform]] = FitTransform
    """
    Class variable pointing to the class object (which should be a subclass of
    :class:`FitTransform`) to use when constructing the result of :meth:`fit()`.
    Subclasses may override this if they want to customize the behavior of the
    ``FitTransform`` instances returned by ``fit()``. Defaults to :class:`FitTransform`.
    """

    backend = LOCAL_BACKEND  # type: Backend

    _visualize_skip_params: ClassVar[Sequence[str] | None] = None
    _visualize_nonparam_attribs: ClassVar[Sequence[str] | None] = None

    pure = True
    """
    Attribute indicating wether this Transform represents a "pure" computation, i.e. one
    that can be cached safely. This may be used by some backends to avoid unnecessary
    recomputations. The default value is ``True``; some subclasses may wish to override
    it with ``False`` if they should always be recomputed whenever applied. For example,
    Transforms that read data, generate random outputs, or have desired side effects.
    """

    # TODO: do we really want tag to be hyperparameterizable? shouldn't it be
    # invariant wrt fit-time data and bindings?
    tag: str = field(
        init=True,
        eq=False,
        kw_only=True,
        default=Factory(_next_auto_tag, takes_self=True),
        repr=True,
    )
    """
    The ``tag`` attribute is the one parameter common to all ``Transforms``. used for
    identifying and selecting Transform instances within Pipelines. Ignored when
    comparing Transforms. It is an optional kwarg to the constructor of ``Transform``
    and all of its subclasses. If not provided, a default numeral value is assigned.
    It's up to the user to keep tags unique within a ``Pipeline``, but the default value
    should generall ensure this.

    .. SEEALSO::
        :data:`name`, :meth:`find_by_name`, :meth:`FitTransform.find_by_name`.
    """

    @tag.validator
    def _check_tag(self, attribute, value):
        if not isinstance(value, str):
            raise TypeError(
                f"tag must be a str-like; but got a {type(value)}: {value!r}"
            )

    @property
    def name(self) -> str:
        """
        The ``name`` of a ``Transform`` is its class name joined by a "``#``" with its
        ``tag`` parameter. For example, a :class:`~frankenfit.dataframe.DeMean` object
        with tag ``"foo"`` has ``name`` ``"DeMean#foo"``. The method
        :meth:`find_by_name` is used to select child ``Transforms`` by ``name``, and
        therefore every ``Transform``'s ``name`` ought to be unique within a
        ``Pipeline``.

        .. SEEALSO:: :data:`tag`, :meth:`find_by_name()`,
            :meth:`FitTransform.find_by_name()`.
        """
        class_name = self.__class__.__qualname__
        return f"{class_name}#{self.tag}"

    def __str__(self) -> str:
        return self.name

    def _fit(self, data_fit: DataType) -> Any:
        """
        Implements subclass-specific fitting logic.

        .. NOTE::
            ``_fit()`` is one of two methods (the other being ``_apply()``) that any
            subclass of :class:`Transform` must implement. (But see
            :class:`StatelessTransform` as a way to avoid this for transforms that don't
            have state to fit.)

        Here are some useful points to keep in mind whilst writing your ``_fit()``
        function, which you can consider part of Frankenfit's API contract:

        - When your ``_fit()`` function is executed, ``self`` actually refers to an
          instance of :class:`FitTransform` (in fact a subclass of ``FitTransform`` that
          is specific to your :class:`Transform` subclass), which is being constructed
          and will store the state that your method returns.
        - TODO: Params all available on self, concrete values, hyperparams resolved.
        - You have access to hyperparameter bindings via :meth:`self.bindings()
          <FitTransform.bindings>`.

        :param df_fit: A pandas ``DataFrame`` of training data.
        :type df_fit: ``pd.DataFrame``
        :raises NotImplementedError: If not implemented by the subclass.
        :return: An arbitrary object, the type and meaning of which are specific to the
            subclass. This object will be passed as the ``state`` argument to
            :meth:`_apply()` at apply-time.
        :rtype: ``object``
        """
        raise NotImplementedError  # pragma: no cover

    def _submit_fit(
        self,
        data_fit: Optional[DataType | Future[DataType]] = None,
        bindings: Optional[Bindings] = None,
    ) -> Any:
        # default implementation of _submit_fit: submit _fit on data_fit (and possibly
        # bindings) to backend.
        sig = inspect.signature(self._fit).parameters
        if len(sig) < 1:
            raise TypeError(
                f"I don't know how to invoke user _fit() method with "
                f"{len(sig)} arguments: {self._fit} with signature "
                f"{str(sig)}"
            )
        additional_args: tuple[Any, ...] = tuple()
        if len(sig) > 1:
            # pass bindings if _fit has a second argument
            additional_args += (bindings or {},)

        with self.parallel_backend() as backend:
            return backend.submit(
                "_fit",
                self._fit,
                backend.maybe_put(data_fit),
                *additional_args,
                pure=self.pure,
            )

    def fit(
        self: R,
        data_fit: Optional[DataType | Future[DataType]] = None,
        bindings: Optional[Bindings] = None,
        /,
        **kwargs,
    ) -> FitTransform[R, DataType]:
        """
        Fit this ``Transform`` on some data and hyperparameter bindings, and return a
        :class:`FitTransform` object.

        Parameters
        ----------
        data_fit: DataType, optional
            The fitting data.

        bindings: Mapping[str, Any], optional
            A mapping from hyperparameter names to values.

        **kwargs:
            Additional hyperparameter bindings as keyword arguments, which take
            precedence over ``bindings``.

        Raises
        ------
        UnresolvedHyperparameterError
            If a required hyperparameter of this ``Transform`` or any of its child
            transforms is not bound by ``bindings`` or ``**kwargs``.
        """
        # Convenience method that fits this transform on a local backend and
        # materializes the state
        return cast(
            FitTransform[R, DataType],
            self.backend.fit(self, data_fit, bindings, **kwargs),
        ).materialize_state()

    def _apply(self, data_apply: DataType, state: Any) -> DataType:
        """
        Implements subclass-specific logic to apply the tansformation after being fit.

        .. NOTE::
            ``_apply()`` is one of two methods (the other being ``_fit()``) that any
            subclass of :class:`Transform` must implement.

        TODO.
        """
        raise NotImplementedError  # pragma: no cover

    def _submit_apply(
        self,
        data_apply: Optional[DataType | Future[DataType]] = None,
        state: Any = None,
    ) -> Future[DataType] | None:
        sig = inspect.signature(self._apply).parameters
        if len(sig) < 2:
            raise TypeError(
                f"I don't know how to invoke user _apply() method with "
                f"{len(sig)} arguments: {self._apply} with signature {str(sig)}"
            )

        with self.parallel_backend() as backend:
            return backend.submit(
                "_apply",
                self._apply,
                backend.maybe_put(data_apply),
                backend.maybe_put(state),
                pure=self.pure,
            )

    def _materialize_state(self, state: Any) -> Any:
        if isinstance(state, Future):
            return state.result()
        return state

    def params(self) -> list[str]:
        """
        Return the names of all of the parameters
        """
        field_names = list(fields_dict(self.__class__).keys())
        return field_names

    def hyperparams(self) -> set[str]:
        """
        Return the set of hyperparameter names that this :class:`Transform` expects to
        be bound at fit-time. If this Transform contains other Transforms (for example
        if it's a :class:`Pipeline` or a :class:`~frankenfit.dataframe.Join`), then the
        set of hyperparameter names is collected recursively.
        """
        # TODO: is there some way to report what type each hyperparam is expected to
        # have?

        sd = SentinelDict()
        sub_transform_results = set()
        for name in self.params():
            unbound_val = getattr(self, name)
            if isinstance(unbound_val, HP):
                try:
                    HP.resolve_maybe(unbound_val, sd)
                except:  # noqa
                    pass
            elif isinstance(unbound_val, Transform):
                sub_transform_results |= unbound_val.hyperparams()
            elif isinstance(unbound_val, list) and len(unbound_val) > 0:
                for x in unbound_val:
                    if isinstance(x, Transform):
                        sub_transform_results |= x.hyperparams()

        return (sd.keys_checked or set()) | sub_transform_results

    def resolve(self: R, bindings: Optional[Bindings] = None) -> R:
        """
        Returns a shallow copy of self with all hyperparameters (i.e.,
        :class:`HP`-valued params) resolved (but not recursively), or raises
        :class:`UnresolvedHyperparameter` if unable to do so with the given bindings.
        """
        bindings = bindings or {}
        params = self.params()
        # TODO someday: check types of bound values against annotations
        resolved_transform = copy.copy(self)
        unresolved = []
        for name in params:
            unbound_val = getattr(self, name)
            bound_val = HP.resolve_maybe(unbound_val, bindings)
            # print("%s: Bound %r -> %r" % (name, unbound_val, bound_val))
            # NOTE: we cannot use setattr because that will trigger attrs'
            # converter machinery, which is only desirable before hyperparams
            # are resolved (columns_field, etc.). So we write to the __dict__
            # directly.
            resolved_transform.__dict__[name] = bound_val
            if isinstance(bound_val, HP):
                unresolved.append(bound_val)

        # freak out if any hyperparameters failed to bind
        if unresolved:
            raise UnresolvedHyperparameterError(
                f"One or more hyperparameters of {self.__class__.__qualname__} were "
                f"not resolved at fit-time: {unresolved}. Bindings were: "
                f"{bindings}"
            )

        return resolved_transform

    def on_backend(self: R, backend: Backend) -> R:
        self_copy = copy.copy(self)
        self_copy.backend = backend
        # TODO: add ourselves to backend name trace?
        return self_copy

    @contextmanager
    def parallel_backend(self) -> Iterator[Backend]:
        with self.backend.submitting_from_transform(self.name) as backend:
            yield backend

    def __eq__(self, other: object) -> bool:
        if type(self) is not type(other):
            return False
        other = cast(Transform, other)
        my_params = sorted([p for p in self.params() if p != "tag"])
        other_params = sorted([p for p in other.params() if p != "tag"])
        # assertion should be true because our types are the same
        assert my_params == other_params
        for p in my_params:
            if getattr(self, p) != getattr(other, p):
                return False
        return True

    def _children(self) -> Iterable[PipelineMember]:
        """
        Recursively searches params for ``Transforms``, ``FitTransforms`` and iterables
        thereof, yielding them in the order found. Subclasses should override this if
        they have other ways of keeping child transforms, so that they are discoverable
        by :meth:`~PipelineMember.find_by_name()`.
        """
        yield self
        for name in self.params():
            val = getattr(self, name)
            if isinstance(val, PipelineMember):
                yield from val._children()
            elif is_iterable(val) and not isinstance(val, str):
                for x in val:
                    if isinstance(x, PipelineMember):
                        yield from x._children()

    def _visualize(self, digraph, bg_fg: tuple[str, str]) -> tuple[list, list]:
        # out of the box, handle three common cases:
        # - we are a simple transform with no child transforms
        # - we have one or more child transforms as Transform-valued params
        # - we have one or more child transforms as elements of a list-valued param
        # Subclasses override for their own cases not covered by the above
        # TODO: this function has gotten too big and needs refactoring
        children_as_params: dict[str, PipelineMember] = {}
        children_as_elements_of_params: dict[str, list[PipelineMember]] = {}
        param_reprs: dict[str, str] = {}

        skip_params = set(self._visualize_skip_params or [])
        nonparam_attribs = set(self._visualize_nonparam_attribs or [])
        params = (set(self.params()) - skip_params) | nonparam_attribs

        for name in params:
            if name == "tag":
                continue
            val = getattr(self, name)
            tvals = []
            has_children = False
            # collect each Transform-type param
            if isinstance(val, PipelineMember):
                children_as_params[name] = val
                has_children = True
            # same for each Transform-type element of a list param
            elif isinstance(val, list) and len(val) > 0:
                for x in val:
                    if isinstance(x, PipelineMember):
                        tvals.append(x)
                        has_children = True
            # for Transform-type values in a dict param
            elif isinstance(val, dict) and len(val) > 0:
                for key, x in val.items():
                    if isinstance(x, PipelineMember):
                        has_children = True
                        children_as_params[f"{name}[{key!r}]"] = x
            if tvals:
                children_as_elements_of_params[name] = tvals
            # for non-Transform params, collect their values to be displayed in the
            # label of the node for this Transform
            if (not has_children) and (val is not None):
                param_reprs[name] = repr(val)

        param_reprs_fmt = ",\n".join(
            [" = ".join([k, v]) for k, v in param_reprs.items()]
        )
        self_label = f"{self.name}\n{param_reprs_fmt}"

        if not (children_as_params or children_as_elements_of_params):
            digraph.node(self.name, label=self_label)
            return ([self.name], [(self.name, "")])

        # we gon' need a cartouche
        my_exits = []
        with digraph.subgraph(name=f"cluster_{self.name}") as sg:
            bg, fg = bg_fg
            bg_fg = fg, bg
            sg.attr(style="filled", color=bg)
            sg.node_attr.update(style="filled", color=fg)
            sg.node(self.name, label=self_label)

            for t_name, t in children_as_params.items():
                t_entries, t_exits = t._visualize(sg, bg_fg)
                for t_entry in t_entries:
                    sg.edge(self.name, t_entry, label=t_name)
                my_exits.extend(t_exits)

            for tlist_name, tlist in children_as_elements_of_params.items():
                prev_exits = None
                for t in tlist:
                    t_entries, t_exits = t._visualize(sg, bg_fg)
                    if prev_exits is None:
                        # edges from self to first transform's entries
                        for t_entry in t_entries:
                            sg.edge(self.name, t_entry, label=tlist_name)
                    else:
                        # edge from prvious transform's exits node to this
                        # transform's entries node
                        for prev_exit, prev_exit_label in prev_exits:
                            for t_entry in t_entries:
                                sg.edge(prev_exit, t_entry, label=prev_exit_label)
                    prev_exits = t_exits
                # last transform in tlist becomes one of our exits
                my_exits.append((t.name, ""))

        return [self.name], my_exits

    def visualize(self, **digraph_kwargs):
        """
        Return a visualization of this Transform as a ``graphviz.DiGraph`` object. The
        caller may render it to file or screen.

        Parameters
        ----------
        digraph_kwargs:
            Additional keyword arguments to pass to ``graphviz.Digraph`` when
            constructing the visualization. Extends and overwrites
            :data:`frankenfit.core.DEFAULT_VISUALIZE_DIGRAPH_KWARGS`.
        """
        digraph = graphviz.Digraph(
            **{**DEFAULT_VISUALIZE_DIGRAPH_KWARGS, **digraph_kwargs}
        )
        self._visualize(digraph, ("lightgrey", "white"))
        return digraph


class SentinelDict(dict):
    """
    Utility class that behaves exactly like an ordinary dict, but keeps track of which
    keys have been read, available in the ``keys_checked`` instance attribute, which is
    either ``None`` if no keys have been read, or the set of keys.
    """

    keys_checked: Optional[set] = None
    default: Any = None

    def _record_key(self, key):
        if self.keys_checked is None:
            self.keys_checked = set()
        self.keys_checked.add(key)

    def __getitem__(self, key):
        self._record_key(key)
        return self.default

    def get(self, key, default=None, /):
        self._record_key(key)
        return default


class StatelessTransform(Generic[DataType], Transform[DataType]):
    """
    Abstract base class of Transforms that have no state to fit.
    :meth:`~Transform.fit()` is a null op on a ``StatelessTransform``, and the
    :meth:`~FitTransform.state()` of its fit is always ``None``. If subclasses implement
    :meth:`~Transform._fit`, they must ensure that it returns ``None``.

    As a convenience, ``StatelessTransform`` has an :meth:`apply()` method
    (ordinarily only the corresponding :class:`FitTransform` would). For any
    ``StatelessTransform`` ``t``::

        t.apply(df, bindings)

    is equivalent to::

        t.fit(df, bindings=bindings).apply(df)

    In particular, if the ``StatelessTransform`` has fit-time and/or apply-time
    side-effects (e.g., printing a message), then ``apply()`` will cause those
    side-effects occur in the expected order.

    .. TIP::

        In the Frankenfit API documentation, stateless Transforms are marked by the
        symbol "üè≥Ô∏è" (unicode "white flag" emoji) with a link back to this class.
    """

    _Self = TypeVar("_Self", bound="StatelessTransform")

    def _fit(self, data_fit: DataType) -> None:
        return None

    def apply(
        self,
        data_apply: Optional[DataType | Future[DataType]] = None,
        bindings: Optional[Bindings] = None,
        /,
        **kwargs,
    ) -> DataType:
        """
        Convenience function allowing one to apply a ``StatelessTransform`` without an
        Explicit preceding call to :meth:`~Transform.fit`. Equivalent to calling
        ``fit()`` on the given data (with the optional hyperparameter bindings as
        provided) and then returning the result of applying the resulting
        :class:`FitTransform` to that same data.
        """
        return self.backend.apply(self, data_apply, bindings, **kwargs).result()


class NonInitialConstantTransformWarning(RuntimeWarning):
    """
    An instance of :class:`ConstantTransform` was found to be non-initial in a
    :class:`Pipeline`, or the user provided it with non-empty input data. This
    is usually unintentional.

    .. SEEALSO::
        :class:`ConstantTransform`
    """


class ConstantTransform(Generic[DataType], StatelessTransform[DataType]):
    """
    Abstract base class of :class:`StatelessTransforms <StatelessTransform>` that have
    no state to fit, and furthermore, at apply time, produce output data that is
    **independent** of the input data. Usually, a ``ConstantTransform`` is some kind of
    data reader or data generator. Its parameters and bindings may influence its output,
    but it takes no input data to be transformed per se.

    Because it has the effect of discarding the output of all preceding computations in
    a :class:`Pipeline`, a warning is emited
    (:class:`NonInitialConstantTransformWarning`) whenever a ``ConstantTransform`` is
    fit on non-empty input data, or found to be non-initial in a :class:`Pipeline`.

    In many cases, a subclass of :class:`ConstantTransform` may want to set its
    :data:`~Transform.pure` attrbute to ``False`` to ensure that backends do not cache
    its output. For example, a data-reading Transform may need to re-read a file on
    every application in case the file has changed since the last application.

    üè≥Ô∏è :class:`Stateless <frankenfit.StatelessTransform>`
    """

    _Self = TypeVar("_Self", bound="ConstantTransform")

    def _submit_fit(
        self: _Self,
        data_fit: Optional[DataType | Future[DataType]] = None,
        bindings: Optional[Bindings] = None,
    ) -> Any:
        if data_fit is not None:
            warning_msg = (
                "A ConstantTransform's fit method received non-null input data. "
                "This is likely unintentional because that input data will be "
                "ignored and discarded.\n"
                f"transform={self!r}\n"
                f"data_fit=\n{data_fit!r}"
            )
            _LOG.warning(warning_msg)
            warnings.warn(
                warning_msg,
                NonInitialConstantTransformWarning,
            )
        return super()._submit_fit(data_fit, bindings)

    def _submit_apply(
        self,
        data_apply: Optional[DataType | Future[DataType]] = None,
        bindings: Optional[Bindings] = None,
    ) -> Future[DataType] | None:
        if data_apply is not None:
            warning_msg = (
                "A ConstantTransform's apply method received non-null input data. "
                "This is likely unintentional because that input data will be "
                "ignored and discarded.\n"
                f"transform={self!r}\n"
                f"data_apply=\n{data_apply!r}"
            )
            _LOG.warning(warning_msg)
            warnings.warn(
                warning_msg,
                NonInitialConstantTransformWarning,
            )
        return super()._submit_apply(data_apply, bindings)


C = TypeVar("C", bound="Callable[..., Any]")


def callchain(transform_class: type[R]) -> Callable[[C], C]:
    def inner(f: C) -> C:
        # f.__doc__ = dedent(f.__doc__ or "") + dedent(transform_class.__doc__ or "")
        if transform_class.__doc__ is not None:
            transform_class.__doc__ += f"""

    .. SEEALSO:: The corresponding call-chain method is :meth:`~{f.__qualname__}()`
"""

        @wraps(f)
        def wrapper(self, *args, **kwargs):
            return self + transform_class(*args, **kwargs)

        return cast(C, wrapper)

    return inner


def method_wrapping_transform(
    class_qualname: str, method_name: str, transform_class: type[R]
) -> Callable[..., Pipeline]:
    def method_impl(self, *args, **kwargs) -> Pipeline:
        return self + transform_class(*args, **kwargs)

    method_impl.__annotations__.update(
        {**transform_class.__init__.__annotations__, **{"return": class_qualname}}
    )
    method_impl.__name__ = method_name
    method_impl.__qualname__ = ".".join((class_qualname, method_name))
    sig = inspect.signature(transform_class.__init__).replace(
        return_annotation=class_qualname
    )
    # Workaround mypy bug: https://github.com/python/mypy/issues/12472
    # method_impl.__signature__ = sig
    setattr(method_impl, "__signature__", sig)
    method_impl.__doc__ = f"""
    Return the result of appending a new :class:`{transform_class.__name__}` transform
    constructed with the given parameters to this pipeline.
    This method's arguments are passed directly to
    ``{transform_class.__name__}.__init__()``.

    .. SEEALSO:: :class:`{transform_class.__qualname__}`
    """
    # if transform_class.__doc__ is not None:
    #     transform_class.__doc__ += f"""

    # .. SEEALSO:: :meth:`{class_qualname}.{method_name}`
    # """

    return method_impl


def _convert_pipeline_transforms(value):
    result = []
    if isinstance(value, Pipeline):
        # "coalesce" Pipelines
        tf_seq = value.transforms
    elif isinstance(value, (Transform, FitTransform)):
        tf_seq = [value]
    elif value is None:  # pragma: no cover
        tf_seq = []
    else:
        tf_seq = list(value)

    for tf_elem in tf_seq:
        if isinstance(tf_elem, Pipeline):
            # "coalesce" Pipelines
            result.extend(tf_elem.transforms)
        elif isinstance(tf_elem, Transform):
            result.append(tf_elem)
        elif isinstance(tf_elem, FitTransform):
            # wrap FitTransforms
            result.append(ApplyFitTransform(tf_elem))
        else:
            raise TypeError(f"Pipeline cannot contain a non-Transform: {tf_elem!r}")

    return result


P_co = TypeVar("P_co", bound="Pipeline", covariant=True)


class Grouper(Generic[P_co]):
    def __init__(
        self,
        pipeline_upstream: P_co,
        wrapper_class: type,
        wrapper_kwarg_name_for_wrappee: str,
        **wrapper_other_kwargs,
    ):
        self._pipeline_upstream = pipeline_upstream
        self._wrapper_class = wrapper_class
        self._wrapper_kwarg_name_for_wrappee = wrapper_kwarg_name_for_wrappee
        self._wrapper_other_kwargs = wrapper_other_kwargs

    def then(self, other: PipelineMember | Sequence[PipelineMember] | None) -> P_co:
        if not isinstance(self._pipeline_upstream, Pipeline):  # pragma: no cover
            raise TypeError(
                f"Grouper cannot be applied to non-Pipeline upstream: "
                f"{self._pipeline_upstream} with type "
                f"{type(self._pipeline_upstream)}"
            )

        if not isinstance(other, Transform):
            other = type(self._pipeline_upstream)(transforms=other)

        wrapping_kwargs = dict(
            **{self._wrapper_kwarg_name_for_wrappee: other},
            **self._wrapper_other_kwargs,
        )
        wrapping_transform = self._wrapper_class(**wrapping_kwargs)
        return self._pipeline_upstream + cast(Transform, wrapping_transform)

    def __add__(self, other: PipelineMember | Sequence[PipelineMember] | None) -> P_co:
        return self.then(other)


@params(auto_attribs=False)
class Pipeline(Generic[DataType], Transform[DataType]):
    transforms: list[Transform] = field(
        factory=list, converter=_convert_pipeline_transforms
    )

    @transforms.validator
    def _check_transforms(self, attribute, value):
        t_is_first = True
        for t in value:
            if not isinstance(t, Transform):
                raise TypeError(
                    "Pipeline sequence must comprise Transform instances; found "
                    f"non-Transform (type {type(t)}): {t!r} "
                )
            # warning if a ConstantTransform is non-initial
            if (not t_is_first) and isinstance(t, ConstantTransform):
                warning_msg = (
                    f"A ConstantTransform is non-initial in a Pipeline: {t!r}. "
                    "This is likely unintentional because the output of all "
                    "preceding Transforms, once computed, will be discarded by "
                    "the ConstantTransform."
                )
                _LOG.warning(warning_msg)
                warnings.warn(
                    warning_msg,
                    NonInitialConstantTransformWarning,
                )
            t_is_first = False

    _pipeline_methods: ClassVar[list[str]] = []

    def _empty_constructor(self):
        return None

    class _Grouper(Grouper):
        pass

    @classmethod
    def with_methods(
        cls: type[P], subclass_name: Optional[str] = None, **kwargs
    ) -> type[P]:
        if subclass_name is None:
            subclass_name = f"{cls.__name__}WithMethods"

        # workaround for mypy bug: https://github.com/python/mypy/issues/5865
        klass: Any = cls

        class Subclass(klass):
            class _Grouper(klass._Grouper):
                pass

            pass

        Subclass.__name__ = subclass_name
        Subclass.__qualname__ = subclass_name
        Subclass._Grouper.__qualname__ = Subclass.__qualname__ + ".Grouper"

        pipeline_methods = []
        if hasattr(cls, "_pipeline_methods"):
            pipeline_methods.extend(cls._pipeline_methods)
        Subclass._pipeline_methods = pipeline_methods
        for method_name, transform_class in kwargs.items():
            setattr(
                Subclass,
                method_name,
                method_wrapping_transform(
                    Subclass.__qualname__, method_name, transform_class
                ),
            )
            Subclass._pipeline_methods.append(method_name)
            setattr(
                Subclass._Grouper,
                method_name,
                method_wrapping_transform(
                    Subclass.__qualname__, method_name, transform_class
                ),
            )

        return Subclass

    def __init__(self, tag=NOTHING, transforms=NOTHING):
        self.__attrs_init__(tag=tag, transforms=transforms)

    def __len__(self):
        return len(self.transforms)

    def _submit_fit(
        self,
        data_fit: DataType | Future[DataType] | None = None,
        bindings: Optional[Bindings] = None,
        return_what: Literal["state", "result"] = "state",
    ) -> list[FitTransform] | Future[DataType]:
        fit_transforms: list[FitTransform] = []
        with self.parallel_backend() as backend:
            data_fit = backend.maybe_put(data_fit)
            n = len(self.transforms)
            if n == 0 and return_what == "result" and data_fit is None:
                return backend.put(self._empty_constructor())
            for (i, t) in enumerate(self.transforms):
                trace = f"[{i}]"
                ft = backend.push_trace(trace).fit(t, data_fit, bindings)
                fit_transforms.append(ft)
                if i < n - 1 or return_what == "result":
                    data_fit = backend.push_trace(trace).apply(ft, data_fit)
                else:
                    # last transform, and we don't need result: no need to apply
                    break

            if return_what == "state":
                return fit_transforms
            else:
                return cast(Future[DataType], data_fit)

    def _submit_apply(
        self,
        data_apply: Optional[DataType | Future[DataType]] = None,
        state: list[FitTransform] | None = None,
    ) -> Future[DataType] | None:
        fit_transforms = state
        assert fit_transforms is not None
        with self.parallel_backend() as backend:
            data_apply = backend.maybe_put(data_apply)
            if len(fit_transforms) == 0 and data_apply is None:
                return backend.put(self._empty_constructor())
            for (i, fit_transform) in enumerate(fit_transforms):
                if isinstance(fit_transform.resolved_transform(), IfPipelineIsFitting):
                    _LOG.debug(
                        "%s._submit_apply: skipping IfPipelineIsFitting instance: %s",
                        self.name,
                        fit_transform.name,
                    )
                    continue
                data_apply = backend.push_trace(f"[{i}]").apply(
                    fit_transform, data_apply
                )
        return cast(Future[DataType], data_apply)

    def apply(
        self,
        data_apply: Optional[DataType | Future[DataType]] = None,
        bindings: Optional[Bindings] = None,
        /,
        **kwargs,
    ) -> DataType:
        """
        An efficient alternative to ``Pipeline.fit(...).apply(...)``. When the fit-time
        data and apply-time data are identical, it is more efficient to use a single
        call to ``apply()`` than it is to call :meth:`~Transform.fit()` followed by a
        separate call to :meth:`~FitTransform.apply()`, both on the same data argument.
        This is because ``fit()`` itself must already apply every transform in the
        pipeline, in orer to produce the fitting data for the following transform.
        ``apply()`` captures the result of these fit-time applications, avoiding their
        unnecessary recomputation.

        :return: The result of fitting this :class:`Pipeline` and applying it to its own
            fitting data.
        """
        return self.backend.apply(self, data_apply, bindings, **kwargs).result()

    def _materialize_state(self, state: Any) -> Any:
        # because a pipeline's state is just a list of FitTransform objects, we
        # may need to materialize its contents
        fit_transforms: list[FitTransform] = super()._materialize_state(state)
        assert isinstance(fit_transforms, list)
        mat_state = [sub_ft.materialize_state() for sub_ft in fit_transforms]
        return mat_state

    def __add__(self: P, other: PipelineMember | Sequence[PipelineMember] | None) -> P:
        return self.then(other)

    def then(
        self: P,
        other: PipelineMember | Sequence[PipelineMember] | None = None,
    ) -> P:
        if other is None:
            transforms = self.transforms
        elif isinstance(other, Pipeline):
            # coalesce pass-through pipeline
            transforms = self.transforms + other.transforms
        elif isinstance(other, (Transform, FitTransform)):
            # we can ignore the type-checker here because we know that
            # _convert_pipeline_transforms() will wrap FitTransform with
            # ApplyFitTransform
            transforms = self.transforms + [other]  # type: ignore [list-item]
        elif isinstance(other, list):
            transforms = self.transforms + other
        else:
            raise TypeError(
                f"I don't know how to extend a Pipeline with {other}, which is of "
                f"type {type(other)}, bases = {type(other).__bases__}. "
            )
        return type(self)(transforms=transforms)

    def if_fitting(self: P, transform: Transform) -> P:
        """
        Append an :class:`~core.IfPipelineIsFitting` transform to this pipeline.
        """
        return self + IfPipelineIsFitting(transform)

    def apply_fit_transform(self: P, fit_transform: FitTransform) -> P:
        """
        Append an :class:`~core.ApplyFitTransform` transform to this pipeline.
        """
        return self + ApplyFitTransform(fit_transform)


@params
class IfPipelineIsFitting(Generic[DataType], Transform[DataType]):
    """
    Apply the given child :class:`Transform` only when the :class:`Pipeline` containing
    this ``IfPipelineIsFitting`` Transform is being fit. This is useful to avoid running
    Transforms that are unnecessary when an alread-fit Pipeline is being applied
    out-fo-sample, for example the preparation of training response columns.

    .. SEEALSO:: The corresponding call-chain method is
        :meth:`~frankenfit.Pipeline.if_fitting()`.

    Parameters
    ----------
    transform: :class:`Transform`
        The transform whose application to restrict to fit-time in the parent pipeline.

    Example
    -------
    Preparing a training response column only at fit-time::

        pipeline = ff.DataFramePipeline()
        my_model = (
            pipeline
            .if_fitting(
                pipeline.assign(
                    price_train=pipeline["price"].pipe(np.log1p).winsorize(0.05)
                )
            )
            .sk_learn(
                ...,
                response_col="price_train"
            )
        )
    """

    transform: Transform[DataType]

    def _submit_fit(
        self,
        data_fit: Optional[DataType | Future[DataType]] = None,
        bindings: Optional[Bindings] = None,
    ) -> Any:
        with self.parallel_backend() as backend:
            return backend.push_trace("then").fit(self.transform, data_fit, bindings)

    def _submit_apply(
        self,
        data_apply: Optional[DataType | Future[DataType]] = None,
        state: Any = None,
    ) -> Future[DataType] | None:
        assert isinstance(state, FitTransform)
        with self.parallel_backend() as backend:
            return backend.push_trace("then").apply(state, data_apply)


@params
class ApplyFitTransform(Generic[R_co, DataType], StatelessTransform[DataType]):
    """
    Wrap an already fit :class:`FitTransform` instance as a stateless Transform.  At
    fit-time, ``ApplyFitTransform`` does nothing; at apply-time, it applies the given
    ``fit_transform`` instance. ``ApplyFitTransform`` therefore serves as a utility
    class for embedding :class:`FitTransform` objects wherever ``Transform`` is
    ordinarily required, for example in a :class:`Pipeline` or as one of the child
    transforms of :class:`~frankenfit.dataframe.Join`,
    :class:`~frankenfit.dataframe.GroupByCols`, etc.

    This is particularly useful in situations where the user has already fit some
    predictive pipeline, and now wants to layer some additional transformations onto its
    input or output. Because those transformations may be stateful themselves, one can
    even create "heterogeneously fit" Pipelines, wherein different parts of the Pipeline
    have been fit on different datasets.

    üè≥Ô∏è :class:`Stateless <frankenfit.StatelessTransform>`

    .. SEEALSO:: The corresponding call-chain method is
        :meth:`~frankenfit.Pipeline.apply_fit_transform()`.

    Note
    ----
    As a syntactic convenience, calling :meth:`Transform.fit()` or
    :meth:`Pipeline.fit()` on a ``FitTransform``-type argument automatically wraps the
    argument in ``ApplyFitTransform``.

    Likewise, :meth:`FitTransform.then(...)` is sugar for
    ``ApplyFitTransform(self).then(...)``.

    Furthermore, if a :class:`Pipeline` instance is constructed with any
    :class:`FitTransform` instances in its ``transforms`` parameter, they are
    automatically wrapped in ``ApplyFitTransform``.

    Parameters
    ----------
    fit_transform: FitTransform
        The :class:`FitTransform` object to wrap.
    """

    fit_transform: FitTransform[R_co, DataType]

    def _submit_apply(
        self,
        data_apply: DataType | Future[DataType] | None = None,
        state: Any = None,
    ) -> Future[DataType] | None:
        with self.parallel_backend() as backend:
            return backend.apply(self.fit_transform, data_apply)
